{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3caa9672",
   "metadata": {},
   "source": [
    "V2: \n",
    "- use GNNs to generalize strategy to arbitrary sized chess graphs\n",
    "    - maybe this will get a general strategy which we can use on 16x16 (so we dont have to train on 16x16 first)\n",
    "- if that works, make multi agent adversarial model for robber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8267ec-ba69-4e2c-a90e-d49bb3052348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.all import graphs\n",
    "import copy, sys, math\n",
    "\n",
    "\n",
    "def available_squares(cop_pos: list, robber_pos: tuple) -> list:\n",
    "    #Function that returns the set of squares that are available to the robber\n",
    "    #return the adjacent vertices of robber_pos \\ adjacent vertices of cops    \n",
    "    c_neighbors = set(cop_pos)\n",
    "    for cop in cop_pos:\n",
    "        c_neighbors = c_neighbors.union(set(G.neighbors(cop)))\n",
    "\n",
    "    if robber_pos == (-1, -1): \n",
    "        return set(G.vertices()) - c_neighbors\n",
    "    else:\n",
    "        r_neighbors = set(G.neighbors(robber_pos)).union({robber_pos})\n",
    "        return r_neighbors - c_neighbors\n",
    "\n",
    "def get_axis(cop_pos, robber_pos):\n",
    "    #returns 'h', 'v', 'posd', 'negd' or none for axis which a cop position occupies\n",
    "    #assuming given correct input ie intersecting but not same posn\n",
    "    if cop_pos == robber_pos:\n",
    "        return 'h'\n",
    "        \n",
    "    if robber_pos[0] == cop_pos[0]:\n",
    "        return 'v'\n",
    "    elif robber_pos[1] == cop_pos[1]:\n",
    "        return 'h'\n",
    "    elif cop_pos[0] + cop_pos[1] == robber_pos[0] + robber_pos[1]:\n",
    "        return 'negd'\n",
    "    elif cop_pos[0] - cop_pos[1] == robber_pos[0] - robber_pos[1]:\n",
    "        return 'posd'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_axes_squares(robber_pos, avail, occupied_axes):\n",
    "    #filters out the squares which are on a cop occupied axis from a list of vertices\n",
    "    for axis in map(lambda a: a[0], set(filter(lambda a: a[1], occupied_axes.items()))):\n",
    "        if axis == 'h':\n",
    "            #remove all with move[1] == robber_pos[1]\n",
    "            avail = set(filter(lambda posn: posn[1] != robber_pos[1], avail))\n",
    "        elif axis == 'v':\n",
    "            #remove all with move[0] == robber_pos[0]\n",
    "            avail = set(filter(lambda posn: posn[0] != robber_pos[0], avail))\n",
    "        elif axis == 'negd':\n",
    "            #remove all w/move[0]+move[1] == robber_pos[0]+robber_pos[1]\n",
    "            avail = set(filter(lambda posn: posn[0] + posn[1] != robber_pos[0] + robber_pos[1], avail))\n",
    "        elif axis == 'posd':\n",
    "            #remove all w/move[0]-move[1] == robber_pos[0]-robber_pos[1]\n",
    "            avail = set(filter(lambda posn: posn[0] - posn[1] != robber_pos[0] - robber_pos[1], avail))\n",
    "    return avail\n",
    "\n",
    "def get_intersecting_squares(cop_pos, robber_pos, occupied_axes):\n",
    "    #compiles a list of the squares which cops can reach that directly attack the robber on a unique line\n",
    "    avail = set(G.neighbors(cop_pos)).union({cop_pos}).intersection(\n",
    "        set(G.neighbors(robber_pos)).union({robber_pos}))\n",
    "    return remove_axes_squares(robber_pos, avail, occupied_axes)\n",
    "\n",
    "def get_SD_length(robber_pos):\n",
    "    #determines the length of the SD\n",
    "    diff = robber_pos[0] - robber_pos[1]\n",
    "    sum = robber_pos[0] + robber_pos[1]\n",
    "    posd_len = len(list(filter(lambda posn: posn[0] - posn[1] == diff, G.vertices())))\n",
    "    negd_len = len(list(filter(lambda posn: posn[0] + posn[1] == sum, G.vertices())))\n",
    "    return min(posd_len, negd_len)\n",
    "\n",
    "def get_min_LSD_move(robber_pos: tuple, moves: list) -> tuple:\n",
    "    #determines the cop config which forces the minimum LSD\n",
    "    min_LSD = sys.maxsize\n",
    "    min_move = tuple()\n",
    "\n",
    "    for move in moves:\n",
    "        r_moves = available_squares(move[0], robber_pos)\n",
    "        LSD = -1\n",
    "\n",
    "        #get LSD of robber for this possible move\n",
    "        for r_move in r_moves:\n",
    "            SD_len = get_SD_length(r_move)\n",
    "            if SD_len > LSD:\n",
    "                LSD = SD_len\n",
    "    \n",
    "    if LSD < min_LSD:\n",
    "        min_LSD = LSD\n",
    "        min_move = move\n",
    "    \n",
    "    return min_move\n",
    "\n",
    "def seenp(move: list, robber_pos: tuple, cop_states: list, robber_states: list)-> bool:\n",
    "    if not (cop_states and robber_states): #empty states given for some reason\n",
    "        return False\n",
    "    \n",
    "    for t in range(len(cop_states) - 1, -1, -1):\n",
    "        if (set(cop_states[t]) == set(move) and robber_states[t] == robber_pos):\n",
    "            print(\"SEEN\")\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "#n^2 algo where n = # available moves, which is constant...\n",
    "def remove_seen_moves(moves: list, robber_pos: tuple, cop_states: list, robber_states: list)-> list:\n",
    "    #removes cop moves from a list of available cop moves if that state has already been seen\n",
    "\n",
    "    if (len(cop_states) != len(robber_states)):\n",
    "        raise Exception(\"given nonmatching states\")\n",
    "    \n",
    "\n",
    "    new_moves = [(m,v) for (m,v) in moves\n",
    "             if not seenp(m, robber_pos, cop_states, robber_states)]\n",
    "\n",
    "    return new_moves\n",
    "\n",
    "def get_closest_unoccupied(cop_pos: list, robber_pos: tuple, idx: int)-> tuple:\n",
    "    V = G.neighbors(cop_pos[idx])\n",
    "\n",
    "    min_dist = sys.maxsize\n",
    "    min_pos = cop_pos[idx]\n",
    "    \n",
    "    for v in V:\n",
    "        dist = math.sqrt((v[0] - robber_pos[0])**2 + (v[1] - robber_pos[1])**2)\n",
    "        if dist < min_dist and v not in cop_pos:\n",
    "            min_dist = dist\n",
    "            min_pos = v\n",
    "\n",
    "    return min_pos\n",
    "\n",
    "def minimize_avail_helper(curr_cop_pos, robber_pos, i, occupied_axes, cop_moves, robber_moves):\n",
    "    #goal is to find the minimizing config of cops\n",
    "    #so track a min config and min robber avail squares\n",
    "    #occupied axes is a dict, represents which robber axes are occupied in current backtracking iteration\n",
    "\n",
    "    #base case i > #cops\n",
    "    if i >= len(curr_cop_pos):\n",
    "        return curr_cop_pos, len(available_squares(curr_cop_pos, robber_pos))\n",
    "\n",
    "    avail = get_intersecting_squares(curr_cop_pos[i], robber_pos, occupied_axes)\n",
    "\n",
    "    moves = list() # list(posn) -> avail_squares\n",
    "\n",
    "    for move in avail:\n",
    "        #find axes this occupies\n",
    "        axis = get_axis(move, robber_pos)\n",
    "        occupied_axes[axis] = True\n",
    "        new_cop_pos = copy.deepcopy(curr_cop_pos)\n",
    "        new_cop_pos[i] = move #move cop i\n",
    "        \n",
    "        curr_config, curr_squares = minimize_avail_helper(new_cop_pos, robber_pos, i+1, occupied_axes, cop_moves, robber_moves)\n",
    "        moves.append((curr_config, curr_squares))\n",
    "    \n",
    "        occupied_axes[axis] = False\n",
    "\n",
    "    #remove all moves which revisit board states\n",
    "    moves = remove_seen_moves(moves, robber_pos, cop_moves, robber_moves)\n",
    "\n",
    "    if not moves:\n",
    "        #animal case-- go closer to the robber\n",
    "        curr_cop_pos[i] = get_closest_unoccupied(curr_cop_pos, robber_pos, i)\n",
    "        return minimize_avail_helper(curr_cop_pos, robber_pos, i+1, occupied_axes, cop_moves, robber_moves)\n",
    "    \n",
    "    #get squares with min # available squares for robber\n",
    "    vals = map(lambda tup: tup[1], moves)\n",
    "    min_val = min(vals)\n",
    "    min_avail_moves = list(filter(lambda tup: tup[1] == min_val, moves))\n",
    "\n",
    "    #sort by which config gives the max min SD\n",
    "    best_move = get_min_LSD_move(robber_pos, min_avail_moves)\n",
    "    \n",
    "    return best_move[0], best_move[1]\n",
    "\n",
    "def minimize_available(cop_pos: list, robber_pos: tuple, cop_states, robber_states) -> list:\n",
    "    # Function that returns the move for the cops that minimizes the number of available squares for the robber\n",
    "    #this function could be the combinatorially large one, but we are going to introduce our greedy heuristic\n",
    "    #our strategy is such: the cops should always directly threaten a unique line of movement\n",
    "    #get set of cop i available_moves \\intersect set of robber \n",
    "    #filter out whichever are on occupied axes\n",
    "    #use backtracking algorithm, recursively call min_avail_helper w/i+1, new cop_pos\n",
    "\n",
    "    occupied_axes = {\n",
    "        'h': False,\n",
    "        'v': False,\n",
    "        'negd': False,\n",
    "        'posd': False\n",
    "    }\n",
    "    \n",
    "    min_config, min_squares = minimize_avail_helper(cop_pos, robber_pos, 0, occupied_axes, cop_states, robber_states)\n",
    "    \n",
    "    return min_config\n",
    "\n",
    "def maximize_available(cop_pos: list, cop_states, robber_states, robber_pos:tuple = (-1, -1)) -> tuple: #-1 denotes no robber placed yet ie startin\n",
    "    # Function that returns move for the robber that maximizes the number of squares for their next turn (assuming cops try to minimize)\n",
    "    #get set of valid moves available_squares\n",
    "    #for all moves m, call available_squares(cop_pos, m), get size of set\n",
    "    #track max size and move, return that move\n",
    "    #O(n)\n",
    "\n",
    "    r_neighbors = available_squares(cop_pos, robber_pos)\n",
    "\n",
    "    moves = dict() # move -> min cop move in anticipation\n",
    "    \n",
    "    for move in r_neighbors:\n",
    "        #Q: here, do we want cop moves to take into account that cops wont repeat moves ?\n",
    "        #at this point, cops are making suboptimal moves\n",
    "        # i guess dont take into account, as robber doesnt care for repeating moves?\n",
    "        cop_response = minimize_available(cop_pos, move, [], [])\n",
    "        max_min_val = len(available_squares(cop_response, move))\n",
    "\n",
    "        moves[move] = max_min_val\n",
    "    \n",
    "    if not moves and robber_pos == (-1, -1):\n",
    "        return (0, 0)\n",
    "    elif not moves and robber_pos != (-1, -1):\n",
    "        return robber_pos\n",
    "\n",
    "    #sort by value descending, then by SD descending\n",
    "    max_val = max(moves.values())\n",
    "    max_avail_moves = {k: v for k, v in moves.items() if v == max_val}\n",
    "\n",
    "    best_moves = sorted(\n",
    "        max_avail_moves.items(),\n",
    "        key=lambda item: -get_SD_length(item[0])\n",
    "    )\n",
    "        \n",
    "    return best_moves[0][0]\n",
    "\n",
    "def k_cop_win(cop_start, robber_start, itr, cop_states, robber_states):\n",
    "    #returns true if cop win possible with k cops\n",
    "    cop_move = minimize_available(cop_start, robber_start, cop_states, robber_states) # The cops try to minimize the available squares\n",
    "    print(\"Cops move:\", cop_move)\n",
    "    cop_states.append(cop_move)\n",
    "    robber_states.append(robber_start)\n",
    "    avail_squares.append(len(available_squares(cop_move, robber_start)))\n",
    "    robber_move = maximize_available(cop_move, cop_states, robber_states, robber_start) # The robber tries to maximize this minimum\n",
    "    print(\"Robber moves to:\", robber_move)\n",
    "    print(avail_squares[-1], \"squares available for after move\", itr)\n",
    "\n",
    "    cop_states.append(cop_move)\n",
    "    robber_states.append(robber_move)\n",
    "    \n",
    "    # Checking if the cops have captured the robber\n",
    "    if len(available_squares(cop_move, robber_move)) == 0:\n",
    "        return True, cop_states, robber_states\n",
    "    \n",
    "    # If the cops can't decrease the number of available moves, they lose\n",
    "    #if len(avail_squares) > 1 and avail_squares[-1] > avail_squares[-2]:\n",
    "    #    print(\"available squares increased\")\n",
    "    #    return False\n",
    "\n",
    "    if itr > n**2:\n",
    "        print(\"iterations exceeded\")\n",
    "        return False, cop_states, robber_states\n",
    "            \n",
    "    # If the cop's haven't won yet, keep going\n",
    "    return k_cop_win(cop_move, robber_move, itr+1, cop_states, robber_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12cf589d-a3cf-4cf6-96d7-e60ffa6d82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE FOR GENERATING ANIMAL/ROYAL GRAPHS GIVEN DIRECTIONS\n",
    "\n",
    "def make_graph(n, slopes, animal=False):\n",
    "    from sage.all import QQ, Infinity\n",
    "\n",
    "    vertices = [(x, y) for x in range(n) for y in range(n)]\n",
    "    G = Graph()\n",
    "    G.add_vertices(vertices)\n",
    "\n",
    "    for i, (x1, y1) in enumerate(vertices):\n",
    "        # Convert slope list to exact rational numbers or Infinity\n",
    "        D = set(QQ(s) if s != 'inf' else Infinity for s in slopes)\n",
    "        for j in range(i+1, len(vertices)):\n",
    "            x2, y2 = vertices[j]\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "\n",
    "            if dx == 0:\n",
    "                slope = Infinity\n",
    "            else:\n",
    "                slope = QQ(dy) / QQ(dx)\n",
    "\n",
    "            if slope in D:\n",
    "                G.add_edge((x1, y1), (x2, y2))\n",
    "                if animal:\n",
    "                    D.remove(slope)\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a884788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "EDIT THIS CODE TO CHANGE THE GRAPH\n",
    "similar to evans code, just input slopes into a list and pass it into make_graph function (specify animal or royal w/bool)\n",
    "'''\n",
    "\n",
    "n=15\n",
    "knight = [2, -2, 1/2, -1/2]\n",
    "queen = [0, 'inf', 1, -1]\n",
    "bishop = [1, -1]\n",
    "idk = [1/3, -1/3, 3, 3]\n",
    "G = make_graph(n, queen, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089929c6-b692-4fe1-a37c-f01727545e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[(5, 5), (9, 7), (7, 9)]\n",
      "rstart: (8, 4), cops: [(5, 5), (9, 7), (7, 9)]\n",
      "Cops move: [(7, 5), (12, 4), (8, 10)]\n",
      "Robber moves to: (6, 2)\n",
      "4 squares available for after move 1\n",
      "Cops move: [(7, 2), (8, 4), (6, 8)]\n",
      "Robber moves to: (2, 6)\n",
      "3 squares available for after move 2\n",
      "Cops move: [(7, 11), (6, 6), (2, 8)]\n",
      "Robber moves to: (5, 3)\n",
      "3 squares available for after move 3\n",
      "Cops move: [(7, 3), (8, 6), (5, 5)]\n",
      "Robber moves to: (1, 7)\n",
      "2 squares available for after move 4\n",
      "Cops move: [(1, 3), (0, 6), (7, 7)]\n",
      "Robber moves to: (8, 0)\n",
      "2 squares available for after move 5\n",
      "Cops move: [(3, 5), (6, 0), (8, 6)]\n",
      "Robber moves to: (12, 4)\n",
      "3 squares available for after move 6\n",
      "Cops move: [(11, 5), (12, 6), (8, 4)]\n",
      "Robber moves to: (9, 1)\n",
      "1 squares available for after move 7\n",
      "Cops move: [(8, 2), (12, 1), (9, 5)]\n",
      "Robber moves to: (14, 6)\n",
      "1 squares available for after move 8\n",
      "Cops move: [(14, 2), (12, 8), (8, 6)]\n",
      "Robber moves to: (9, 1)\n",
      "2 squares available for after move 9\n",
      "SEEN\n",
      "Cops move: [(8, 2), (9, 11), (13, 1)]\n",
      "Robber moves to: (12, 4)\n",
      "1 squares available for after move 10\n",
      "Cops move: [(11, 5), (9, 4), (12, 0)]\n",
      "Robber moves to: (14, 6)\n",
      "1 squares available for after move 11\n",
      "Cops move: [(10, 6), (9, 11), (14, 0)]\n",
      "Robber moves to: (13, 5)\n",
      "1 squares available for after move 12\n",
      "Cops move: [(12, 6), (3, 5), (13, 1)]\n",
      "Robber moves to: (10, 2)\n",
      "1 squares available for after move 13\n",
      "Cops move: [(8, 2), (7, 5), (10, 4)]\n",
      "Robber moves to: (14, 6)\n",
      "1 squares available for after move 14\n",
      "Cops move: [(14, 2), (8, 6), (13, 7)]\n",
      "Robber moves to: (9, 1)\n",
      "1 squares available for after move 15\n",
      "SEEN\n",
      "SEEN\n",
      "Cops move: [(9, 7), (8, 2), (13, 1)]\n",
      "Robber moves to: (14, 6)\n",
      "1 squares available for after move 16\n",
      "SEEN\n",
      "SEEN\n",
      "SEEN\n",
      "SEEN\n",
      "SEEN\n",
      "SEEN\n",
      "Cops move: [(9, 11), (14, 2), (8, 6)]\n",
      "Robber moves to: (13, 5)\n",
      "1 squares available for after move 17\n",
      "SEEN\n",
      "Cops move: [(3, 5), (14, 4), (13, 1)]\n",
      "Robber moves to: (10, 2)\n",
      "1 squares available for after move 18\n",
      "Cops move: [(7, 5), (10, 0), (14, 2)]\n",
      "Robber moves to: (11, 3)\n",
      "1 squares available for after move 19\n",
      "Cops move: [(8, 6), (11, 1), (13, 3)]\n",
      "Robber moves to: (11, 3)\n",
      "0 squares available for after move 20\n",
      "Cop win: True\n"
     ]
    }
   ],
   "source": [
    "#n = 15\n",
    "#T = n**2\n",
    "#G = graphs.QueenGraph([n,n])\n",
    "\n",
    "'''\n",
    "run this code to run the above greedy algorithm on the graph defined above\n",
    "define a list of tuples representing where you want your cops to start in (x,y) coords\n",
    "then pass into play_game()\n",
    "\n",
    "you could also use this to iteratively check the largest n for which k cops can win w/this algorithm in a loop\n",
    "'''\n",
    "\n",
    "avail_squares = list()\n",
    "\n",
    "def play_game(cops_start):\n",
    "    print(cops_start)\n",
    "    robber_start = maximize_available(cops_start.copy(), [], [])\n",
    "    robber_moves = [robber_start]\n",
    "    cop_moves = [cops_start]\n",
    "    print(f\"rstart: {robber_start}, cops: {cops_start}\")\n",
    "    return k_cop_win(cops_start, robber_start, 1, cop_moves, robber_moves)\n",
    "\n",
    "#6x6 domination\n",
    "dom_start = math.floor((n+1)/2) - 3\n",
    "dom_set = [(dom_start, dom_start), (dom_start + 4, dom_start + 2), (dom_start + 2, dom_start + 4)]\n",
    "\n",
    "corner_start = [(0,0), (n-1,n-1), (0,n-1)]\n",
    "\n",
    "two_cops = [(0,0), (n-1,n-1)]\n",
    "\n",
    "mid = math.floor(n/2)\n",
    "print(mid)\n",
    "four_cops = [(mid, mid), (mid-1, mid), (mid-1,mid-1), (mid,mid-1)]\n",
    "\n",
    "knight_diag = [(mid, mid), (mid+1, mid+1), (mid-1,mid-1)]\n",
    "\n",
    "winp, cop_moves, robber_moves = play_game(dom_set)\n",
    "print(\"Cop win:\", winp)\n",
    "#print(cop_moves, robber_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28fde1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipyevents in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: ipywidgets>=7.6.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipyevents) (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (8.36.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (4.13.2)\n",
      "Requirement already satisfied: wcwidth in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipyevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9384dd-bc7e-4e5e-aa9a-a7266057b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.all import graphs\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "from ipyevents import Event\n",
    "\n",
    "\n",
    "def get_state(r_state, c_state):\n",
    "    #get dict w/red = occupied by cops, blue = cops, black = robber, green = available for robber movement\n",
    "    cop_occ = set()\n",
    "    for cop in c_state:\n",
    "        cop_occ = cop_occ.union(set(G.neighbors(cop)))\n",
    "\n",
    "    cop_occ -= set(c_state)\n",
    "    \n",
    "    state = {\n",
    "        'blue': set(c_state),\n",
    "        'black': {r_state},\n",
    "        'green': set(available_squares(c_state, r_state)) - {r_state},\n",
    "        'red': cop_occ - {r_state}\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def convert_to_state(robber_moves, cop_moves):\n",
    "    if len(robber_moves) != len(cop_moves):\n",
    "        raise Exception(\"nonequal lists given\")\n",
    "    \n",
    "    states = list()\n",
    "\n",
    "    for state in range(len(robber_moves)):\n",
    "        states.append(get_state(robber_moves[state], cop_moves[state]))\n",
    "\n",
    "    return states\n",
    "\n",
    "# Update function for each turn\n",
    "def update(turn, states, nx_G, pos):\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    nx.draw(nx_G, pos, ax=ax, node_color='lightgrey', node_size=400, with_labels=False)\n",
    "\n",
    "    for color, nodes in states[turn].items():\n",
    "        nx.draw_networkx_nodes(nx_G, pos, nodelist=list(nodes), node_color=color, node_size=400, ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Turn {math.floor(turn / 2) + 1}\")\n",
    "    ax.set_axis_off()\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "run this code to visualize moves made\n",
    "'''\n",
    "\n",
    "def display_game(G, cop_moves, robber_moves):\n",
    "    n = int(math.sqrt(len(list(G.vertices()))))\n",
    "\n",
    "    pos = {(i, j): (i, j) for i in range(n) for j in range(n)}\n",
    "    G.set_pos(pos)\n",
    "    nx_G = G.networkx_graph()\n",
    "\n",
    "    states = convert_to_state(robber_moves, cop_moves)\n",
    "\n",
    "    slider = widgets.IntSlider(min=0, max=len(states) - 1, step=1, value=0)\n",
    "    \n",
    "    out = widgets.interactive_output(\n",
    "        update,\n",
    "        {\n",
    "            'turn': slider,\n",
    "            'states': widgets.fixed(states),\n",
    "            'nx_G': widgets.fixed(nx_G),\n",
    "            'pos': widgets.fixed(pos)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    event = Event(source=slider, watched_events=['keydown'])\n",
    "\n",
    "    def handle_event(event):\n",
    "        if event['key'] == 'ArrowRight':\n",
    "            slider.value = min(slider.max, slider.value + slider.step)\n",
    "        elif event['key'] == 'ArrowLeft':\n",
    "            slider.value = max(slider.min, slider.value - slider.step)\n",
    "\n",
    "    event.on_dom_event(handle_event)\n",
    "\n",
    "    display(slider, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6698de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (2.37.0)\n",
      "Requirement already satisfied: numpy in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from imageio) (2.2.6)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from imageio) (11.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "def save_game_gif(G, cop_moves, robber_moves, gif_path=\"game.gif\", dpi=100):\n",
    "    n = int(math.sqrt(len(list(G.vertices()))))\n",
    "    pos = {(i, j): (i, j) for i in range(n) for j in range(n)}\n",
    "    G.set_pos(pos)\n",
    "    nx_G = G.networkx_graph()\n",
    "    states = convert_to_state(robber_moves, cop_moves)\n",
    "\n",
    "    temp_dir = \"frames_temp\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    frame_paths = []\n",
    "\n",
    "    for i in range(len(states)):\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        nx.draw(nx_G, pos, ax=ax, node_color='lightgrey', node_size=400, with_labels=False)\n",
    "\n",
    "        for color, nodes in states[i].items():\n",
    "            nx.draw_networkx_nodes(nx_G, pos, nodelist=list(nodes), node_color=color, node_size=400, ax=ax)\n",
    "\n",
    "        ax.set_title(f\"Turn {math.floor(i / 2) + 1}\")\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        frame_path = os.path.join(temp_dir, f\"frame_{i:03d}.png\")\n",
    "        fig.savefig(frame_path, dpi=dpi)\n",
    "        frame_paths.append(frame_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Stitch frames into GIF\n",
    "    images = [imageio.imread(p) for p in frame_paths]\n",
    "    imageio.mimsave(gif_path, images, fps=3)\n",
    "\n",
    "    # Clean up\n",
    "    for p in frame_paths:\n",
    "        os.remove(p)\n",
    "    os.rmdir(temp_dir)\n",
    "\n",
    "    print(f\"GIF saved to {gif_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa566fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb6f74880814e44a2a3ef446021d8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, max=40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6469018831fc41d79a0a72159b93e13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save_game_gif(G, cop_moves, robber_moves, gif_path=\"game8.gif\")\n",
    "display_game(G, cop_moves, robber_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab9644",
   "metadata": {},
   "source": [
    "REINFORCEMENT LEARNING MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a17529f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from triton==3.3.0->torch) (80.8.0)\n",
      "Requirement already satisfied: numpy in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch-geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Using cached aiohttp-3.12.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: fsspec in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch-geometric) (2025.5.1)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch-geometric) (2.2.6)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch-geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch-geometric) (4.67.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->torch-geometric)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Using cached multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch-geometric)\n",
      "  Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch-geometric)\n",
      "  Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from requests->torch-geometric) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from requests->torch-geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from requests->torch-geometric) (2025.4.26)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Using cached aiohttp-3.12.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch-geometric\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [torch-geometric] [torch-geometric]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.7 aiosignal-1.3.2 frozenlist-1.6.0 multidict-6.4.4 propcache-0.3.1 torch-geometric-2.6.1 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "102c1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def sage_to_pyg_data(G_sage, cop_positions: list[tuple], robber_position: tuple):\n",
    "    \"\"\"\n",
    "    Convert a Sage Graph (with vertices=(i,j) for i,j in [0..n-1]) into a PyG Data:\n",
    "      - edge_index: shape [2, E]\n",
    "      - x:         shape [n^2, 2] where x[v,0]=1 if a cop is on v, x[v,1]=1 if robber on v.\n",
    "    \"\"\"\n",
    "    # 1) Extract and sort all vertices (cast Sage Integers → int):\n",
    "    vertices = list(G_sage.vertices())               # e.g. [(0,0),(0,1),...]\n",
    "    vertices = [(int(a), int(b)) for (a, b) in vertices] #cast\n",
    "    vertices.sort(key=lambda v: (v[0], v[1]))         # ensure lex order\n",
    "\n",
    "    n2 = len(vertices)                               # should be n*n\n",
    "    node2idx = { vert: idx for idx, vert in enumerate(vertices) }\n",
    "\n",
    "    # 2) Build edge_index lists:\n",
    "    row, col = [], []\n",
    "    for (i, j) in vertices:\n",
    "        v_idx = node2idx[(i, j)]\n",
    "        for nbr in G_sage.neighbors((i, j)):\n",
    "            i2, j2 = int(nbr[0]), int(nbr[1])\n",
    "            u_idx = node2idx[(i2, j2)]\n",
    "            row.append(v_idx)\n",
    "            col.append(u_idx)\n",
    "    edge_index = torch.tensor([row, col], dtype=torch.long)  # [2, num_edges]\n",
    "\n",
    "    # 3) Build node‐feature matrix x of shape [n2, 2]:\n",
    "    x = torch.zeros((n2, 2), dtype=torch.float)\n",
    "    for (ci, cj) in cop_positions:\n",
    "        idx_c = node2idx[(ci, cj)]\n",
    "        x[idx_c, 0] = 1\n",
    "    (ri, rj) = robber_position\n",
    "    idx_r = node2idx[(ri, rj)]\n",
    "    x[idx_r, 1] = 1\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87a653e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== classic_control =====\n",
      "Acrobot-v1             CartPole-v0            CartPole-v1\n",
      "MountainCar-v0         MountainCarContinuous-v0 Pendulum-v1\n",
      "===== phys2d =====\n",
      "phys2d/CartPole-v0     phys2d/CartPole-v1     phys2d/Pendulum-v0\n",
      "===== box2d =====\n",
      "BipedalWalker-v3       BipedalWalkerHardcore-v3 CarRacing-v3\n",
      "LunarLander-v3         LunarLanderContinuous-v3\n",
      "===== toy_text =====\n",
      "Blackjack-v1           CliffWalking-v0        FrozenLake-v1\n",
      "FrozenLake8x8-v1       Taxi-v3\n",
      "===== tabular =====\n",
      "tabular/Blackjack-v0   tabular/CliffWalking-v0\n",
      "===== mujoco =====\n",
      "Ant-v2                 Ant-v3                 Ant-v4\n",
      "Ant-v5                 HalfCheetah-v2         HalfCheetah-v3\n",
      "HalfCheetah-v4         HalfCheetah-v5         Hopper-v2\n",
      "Hopper-v3              Hopper-v4              Hopper-v5\n",
      "Humanoid-v2            Humanoid-v3            Humanoid-v4\n",
      "Humanoid-v5            HumanoidStandup-v2     HumanoidStandup-v4\n",
      "HumanoidStandup-v5     InvertedDoublePendulum-v2 InvertedDoublePendulum-v4\n",
      "InvertedDoublePendulum-v5 InvertedPendulum-v2    InvertedPendulum-v4\n",
      "InvertedPendulum-v5    Pusher-v2              Pusher-v4\n",
      "Pusher-v5              Reacher-v2             Reacher-v4\n",
      "Reacher-v5             Swimmer-v2             Swimmer-v3\n",
      "Swimmer-v4             Swimmer-v5             Walker2d-v2\n",
      "Walker2d-v3            Walker2d-v4            Walker2d-v5\n",
      "===== None =====\n",
      "GymV21Environment-v0   GymV26Environment-v0\n",
      "===== gymnasium_env =====\n",
      "gymnasium_env/CopsAndRobbers-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/envs/registration.py:644: UserWarning: \u001b[33mWARN: Overriding environment gymnasium_env/CopsAndRobbers-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Dict, MultiDiscrete, Tuple, Discrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CopsAndRobbersEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for Cops and Robbers on an nxn grid or graph.\n",
    "    currently from the pov of the cops\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, k, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Inputs ---\n",
    "        self.k = k\n",
    "        self.graph = graph  #SageMath graph\n",
    "        self.nodes = list(self.graph.vertices())\n",
    "        self.n = math.sqrt(len(self.nodes))\n",
    "        self.last_num_avail = 99999\n",
    "        max_deg = int(self.get_max_deg() + 1)\n",
    "\n",
    "        #print(type(max_deg))\n",
    "\n",
    "        # 1) Observation space: dict with\n",
    "        #    - \"cop_pos\": flattened k (x,y) pairs\n",
    "        #    - \"robber_pos\": single (x,y) pair\n",
    "        self.observation_space = Dict({\n",
    "            \"cop_pos\": MultiDiscrete([self.n, self.n] * self.k),   # [x1,y1, x2,y2, …, xk,yk]\n",
    "            \"robber_pos\": MultiDiscrete([self.n, self.n])     # [xr,yr]\n",
    "        })\n",
    "\n",
    "        # 2) Action space: a tuple of k Discrete spaces, each of size (max_deg+1)\n",
    "        #    (we’ll map 0…max_deg-1 to “move to the i‑th neighbor” and max_deg to “stay”)\n",
    "        self.action_space = Tuple([Discrete(max_deg) for _ in range(self.k)])\n",
    "\n",
    "        self.cop_pos = [(-1, -1)] * self.k #list of k tuples representing cop positions\n",
    "        self.robber_pos = (-1, -1)\n",
    "        self.render_mode = render_mode\n",
    "        self.itr = 0\n",
    "\n",
    "    def get_max_deg(self):\n",
    "        max_degree = 0\n",
    "\n",
    "        for vertex in self.graph.vertices():\n",
    "            degree = self.graph.degree(vertex)\n",
    "            if degree > max_degree:\n",
    "                max_degree = degree\n",
    "        return max_degree\n",
    "\n",
    "    def get_collective_euclidean_dist(self):\n",
    "        total_dist = 0\n",
    "        for cop in self.cop_pos:\n",
    "            total_dist += math.sqrt((cop[0] - self.robber_pos[0])**2 + (cop[1] - self.robber_pos[1])**2)\n",
    "        return total_dist\n",
    "    \n",
    "    '''\n",
    "    get the current observations, should be robber and cop positions i think\n",
    "    '''\n",
    "    def get_obs(self):\n",
    "        # Flatten your list of k (x,y) tuples into a single list:\n",
    "        flat_cops = [coord for pos in self.cop_pos for coord in pos]\n",
    "        # Robber is a single (x,y)\n",
    "        #print(self.robber_pos)\n",
    "        rob = list(self.robber_pos)\n",
    "        return {\n",
    "            \"cop_pos\": np.array(flat_cops, dtype=int),\n",
    "            \"robber_pos\": np.array(rob, dtype=int)\n",
    "        }\n",
    "    \n",
    "    def get_info(self):\n",
    "        '''\n",
    "        return info pertinent to reward\n",
    "        Q: what do we care about?\n",
    "        A: i think\n",
    "            - robber curr LSD length / layer (jacobs idea)\n",
    "            - #available squares for robber?\n",
    "        maybe define reward fn first\n",
    "        '''\n",
    "        return {\n",
    "            \"distance\": 1,\n",
    "            \"SD_length\": 1,\n",
    "            \"robber_available:\": 1\n",
    "        }\n",
    "\n",
    "    def reset(self, seed = None, options = None):       \n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state and return the initial observation.\n",
    "        random start?\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        #random for now, start doesnt matter\n",
    "        for _ in range(self.k):\n",
    "            self.cop_pos = random.sample(self.nodes, self.k)\n",
    "\n",
    "        self.robber_pos = maximize_available(self.cop_pos, [], []) #pick maximizing available start for robber\n",
    "        self.itr = 0\n",
    "        self.last_num_avail = available_squares(self.cop_pos, self.robber_pos)\n",
    "        \n",
    "        # Return initial state\n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action (cop move) and return next observation, reward, terminated, truncated, info.\n",
    "        input is an action in the form of a k-tuple, each from 0 to max_deg\n",
    "        recall for cop alpha, a value of i means move to the ith neighbors, a value of max_deg means stay\n",
    "        \"\"\"\n",
    "        assert self.action_space.contains(action), \"Invalid action!\"\n",
    "\n",
    "        # Check terminal state first! to catch dominating start \n",
    "        terminated = not available_squares(self.cop_pos, self.robber_pos)\n",
    "        truncated = self.itr > max(self.n**2, 100)\n",
    "\n",
    "        if not terminated and not truncated: #make action for both cops and robbers\n",
    "            self.itr += 1\n",
    "    \n",
    "            for i, act in enumerate(action):\n",
    "                current_pos = self.cop_pos[i]\n",
    "                neighbors = self.graph.neighbors(current_pos)\n",
    "    \n",
    "                if act < len(neighbors):\n",
    "                    self.cop_pos[i] = neighbors[act]  #move to ith neighbor\n",
    "                else:\n",
    "                    pass  #stay in place\n",
    "    \n",
    "            #self.robber_pos = maximize_available(self.cop_pos, [], [], self.robber_pos)\n",
    "            choices = list(available_squares(self.cop_pos, self.robber_pos))\n",
    "            self.robber_pos = random.sample(choices, 1)[0] if choices else self.robber_pos\n",
    "    \n",
    "        # Define reward\n",
    "        reward = float()\n",
    "        if terminated:\n",
    "            reward = 50000 #reward for catching\n",
    "        elif truncated:\n",
    "            reward = -100  #penalize running out of time\n",
    "        else:\n",
    "            #small negative reward each step to encourage faster capture\n",
    "            reward -= 0.1\n",
    "\n",
    "            #penalize robber mobility — less mobility is better for cops\n",
    "            avail = available_squares(self.cop_pos, self.robber_pos)\n",
    "            reward -= len(avail)  # weight this — tune as needed\n",
    "\n",
    "            # Penalize collective distance from cops to robber\n",
    "            dist_penalty = self.get_collective_euclidean_dist()\n",
    "            reward -= 0.5 * dist_penalty  # again, weight can be tuned\n",
    "\n",
    "            #penalize cops on the same square\n",
    "            if len(self.cop_pos) != len(set(self.cop_pos)):\n",
    "                reward -= 100\n",
    "            \n",
    "            if avail < self.last_num_avail:\n",
    "                reward += 100\n",
    "            \n",
    "            self.last_num_avail = avail\n",
    "\n",
    "    \n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()        \n",
    "\n",
    "        #self.render()\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        visualize the environment\n",
    "        do it with above defined fns\n",
    "        \"\"\"\n",
    "        if self.render_mode == \"human\":\n",
    "            print(f\"Cops at {self.cop_pos}, Robber at {self.robber_pos}, iteration {self.itr}\")\n",
    "        \n",
    "    def get_moves(self):\n",
    "        return self.cop_pos, self.robber_pos\n",
    "    \n",
    "\n",
    "class GNNCopsAndRobbersEnv(CopsAndRobbersEnv):\n",
    "    \"\"\"\n",
    "    Subclass of your original Sage‐based environment.\n",
    "    Overrides reset() and step() to return a PyG Data instead of a numpy dict.\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, k, render_mode=None):\n",
    "        super().__init__(graph=graph, k=k, render_mode=render_mode)\n",
    "        # self.graph is the Sage graph (e.g. QueenGraph([n,n]))\n",
    "        # self.cop_pos, self.robber_pos are managed by the parent class.\n",
    "\n",
    "    def get_pyg_data(self):\n",
    "        return sage_to_pyg_data(self.graph, self.cop_pos, self.robber_pos)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs_dict, info = super().reset(seed=seed, options=options)\n",
    "        # Now self.cop_pos, self.robber_pos have been initialized by the parent.\n",
    "        data = self.get_pyg_data()\n",
    "        return data, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs_dict, reward, terminated, truncated, info = super().step(action)\n",
    "        data = self.get_pyg_data()\n",
    "        return data, reward, terminated, truncated, info\n",
    "\n",
    "gym.register(\n",
    "    id=\"gymnasium_env/CopsAndRobbers-v0\",\n",
    "    entry_point=CopsAndRobbersEnv,\n",
    ")\n",
    "gym.pprint_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93b72325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GNNCopPolicy(nn.Module):\n",
    "    def __init__(self, in_feats=2, hidden_dim=64, emb_dim=64):\n",
    "        \"\"\"\n",
    "        in_feats:  number of node features (2 = [is_cop, is_robber]).\n",
    "        hidden_dim: intermediate GCN dimension.\n",
    "        emb_dim:     final node embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 1) Two-layer GCN\n",
    "        self.conv1 = GCNConv(in_feats, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, emb_dim)\n",
    "\n",
    "        # 2) Policy head: MLP that scores (h_v || h_u) → scalar logit\n",
    "        self.policy_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, 1)\n",
    "        )\n",
    "        # 3) Value head: map a pooled graph embedding → V(s)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data: Data, cop_positions: list[tuple]):\n",
    "        \"\"\"\n",
    "        data.x         : [n_nodes, in_feats]\n",
    "        data.edge_index: [2, n_edges]\n",
    "        cop_positions  : list of k (i,j) pairs\n",
    "\n",
    "        Returns:\n",
    "          - action_logits: a list of length k; each is a 1D FloatTensor of size (|neighbors of that cop| + 1).\n",
    "          - state_value:   a single‐element FloatTensor giving V(s).\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index  # x: [n², 2]\n",
    "\n",
    "        # 1) GCN message‐passing → x_emb: [n², emb_dim]\n",
    "        h = F.relu(self.conv1(x, edge_index))   # [n², hidden_dim]\n",
    "        h = self.conv2(h, edge_index)           # [n², emb_dim]\n",
    "\n",
    "        # 2) Critic: pool all node embeddings (simple mean) → graph embedding [emb_dim]\n",
    "        graph_embed = h.mean(dim=0)              # [emb_dim]\n",
    "        state_value = self.value_head(graph_embed)  # [1]\n",
    "\n",
    "        # 3) Actor: for each cop i, gather its node idx and neighbor idxs\n",
    "        action_logits = []\n",
    "        num_nodes = data.x.size(0)\n",
    "        board_n = int(math.sqrt(num_nodes))  # since num_nodes = n²\n",
    "\n",
    "        for (ci, cj) in cop_positions:\n",
    "            v_idx = ci * board_n + cj           # integer node‐index\n",
    "            h_v = h[v_idx]                      # [emb_dim]\n",
    "\n",
    "            # Find all neighbors of v_idx:\n",
    "            # In PyG, edge_index[0] == v_idx → edge_index[1] are neighbors.\n",
    "            mask_src = (edge_index[0] == v_idx)\n",
    "            nbr_idxs = edge_index[1, mask_src].unique()  # 1D tensor of neighbor indices\n",
    "\n",
    "            # Build a candidate list: “stay” (h_v) + each neighbor embedding h_u\n",
    "            candidate_embs = [h_v] + [h[int(u)] for u in nbr_idxs]\n",
    "            logits = []\n",
    "            for h_u in candidate_embs:\n",
    "                pair = torch.cat([h_v, h_u], dim=-1)    # [2*emb_dim]\n",
    "                score = self.policy_mlp(pair)           # [1]\n",
    "                logits.append(score)\n",
    "            logits = torch.cat(logits, dim=0)  # size = (#neighbors+1)\n",
    "            action_logits.append(logits)\n",
    "\n",
    "        return action_logits, state_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4020f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_reinforce(env: GNNCopsAndRobbersEnv,\n",
    "                    policy: GNNCopPolicy,\n",
    "                    num_episodes=5000,\n",
    "                    gamma=0.99,\n",
    "                    lr=1e-3):\n",
    "    \"\"\"\n",
    "    A bare‐bones REINFORCE trainer.\n",
    "    env         : GNNCopsAndRobbersEnv\n",
    "    policy      : GNNCopPolicy\n",
    "    num_episodes: how many episodes to sample\n",
    "    gamma       : discount factor\n",
    "    lr          : learning rate for Adam\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        data, info = env.reset()               # data: PyG Data with x & edge_index\n",
    "        cop_positions = env.cop_pos            # list of k (i,j) pairs\n",
    "        log_probs = []                         # will store sum of log‐probs per timestep\n",
    "        rewards = []                           # will store rewards\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # 1) Forward pass: get a list of logits per cop + state value\n",
    "            action_logits_list, _ = policy(data, cop_positions)\n",
    "            total_logprob = 0.0\n",
    "\n",
    "            # 2) Sample one action per cop\n",
    "            actions = []\n",
    "            for logits in action_logits_list:\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                m = torch.distributions.Categorical(probs)\n",
    "                a_i = m.sample()\n",
    "                total_logprob = total_logprob + m.log_prob(a_i)\n",
    "                actions.append(int(a_i.item()))\n",
    "\n",
    "            log_probs.append(total_logprob)\n",
    "\n",
    "            # 3) Convert “per‐cop index” → actual (i,j) moves\n",
    "            chosen_moves = []\n",
    "            num_nodes = data.x.size(0)\n",
    "            board_n = int(math.sqrt(num_nodes))\n",
    "\n",
    "            for i, a_i in enumerate(actions):\n",
    "                (ci, cj) = cop_positions[i]\n",
    "                v_idx = ci * board_n + cj\n",
    "                mask_src = (data.edge_index[0] == v_idx)\n",
    "                nbr_idxs = data.edge_index[1, mask_src].unique()\n",
    "\n",
    "                if a_i == 0:\n",
    "                    new_idx = v_idx   # stay\n",
    "                else:\n",
    "                    new_idx = int(nbr_idxs[a_i - 1])\n",
    "                new_i = new_idx // board_n\n",
    "                new_j = new_idx % board_n\n",
    "                chosen_moves.append((new_i, new_j))\n",
    "\n",
    "            # 4) Step environment\n",
    "            data_next, r, terminated, truncated, info = env.step(chosen_moves)\n",
    "            rewards.append(r)\n",
    "\n",
    "            data = data_next\n",
    "            cop_positions = env.cop_pos   # updated internally by env.step()\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # 5) Episode ended: compute discounted returns\n",
    "        Gt = 0.0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            Gt = r + gamma * Gt\n",
    "            returns.insert(0, Gt)\n",
    "        returns = torch.tensor(returns, dtype=torch.float)\n",
    "\n",
    "        # 6) Normalize returns (optional but often helpful)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # 7) Compute policy loss = -∑_t [ logπ(a_t|s_t) * R_t ]\n",
    "        policy_loss = 0.0\n",
    "        for logp, R in zip(log_probs, returns):\n",
    "            policy_loss = policy_loss - (logp * R)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 8) Logging every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_return = sum(rewards) / len(rewards)\n",
    "            print(f\"Episode {episode+1:4d} | AvgReturn (this ep): {avg_return:.2f}\")\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6007f730",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't assign a sage.rings.integer.Integer to a torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m policy \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# or \"cpu\" if no GPU\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 6.4) Train with REINFORCE\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m trained_policy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_reinforce\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRealNumber\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0.99\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRealNumber\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1e-3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 6.5) Evaluate 5 episodes\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Integer(\u001b[38;5;241m5\u001b[39m)):\n",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m, in \u001b[0;36mtrain_reinforce\u001b[0;34m(env, policy, num_episodes, gamma, lr)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 20\u001b[0m     data, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m               \u001b[38;5;66;03m# data: PyG Data with x & edge_index\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     cop_positions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mcop_pos            \u001b[38;5;66;03m# list of k (i,j) pairs\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m []                         \u001b[38;5;66;03m# will store sum of log‐probs per timestep\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 195\u001b[0m, in \u001b[0;36mGNNCopsAndRobbersEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    193\u001b[0m obs_dict, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mseed, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Now self.cop_pos, self.robber_pos have been initialized by the parent.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pyg_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, info\n",
      "Cell \u001b[0;32mIn[38], line 190\u001b[0m, in \u001b[0;36mGNNCopsAndRobbersEnv.get_pyg_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_pyg_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msage_to_pyg_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcop_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobber_pos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 33\u001b[0m, in \u001b[0;36msage_to_pyg_data\u001b[0;34m(G_sage, cop_positions, robber_position)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (ci, cj) \u001b[38;5;129;01min\u001b[39;00m cop_positions:\n\u001b[1;32m     32\u001b[0m     idx_c \u001b[38;5;241m=\u001b[39m node2idx[(ci, cj)]\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m Integer(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m (ri, rj) \u001b[38;5;241m=\u001b[39m robber_position\n\u001b[1;32m     35\u001b[0m idx_r \u001b[38;5;241m=\u001b[39m node2idx[(ri, rj)]\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a sage.rings.integer.Integer to a torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "# 6.1) Build an arbitrary Sage graph (e.g. 8×8 queen graph):\n",
    "from sage.all import graphs\n",
    "G8 = graphs.QueenGraph([8, 8])  # keeps your existing Sage code\n",
    "\n",
    "# 6.2) Wrap it in our GNN‐compatible env\n",
    "env = GNNCopsAndRobbersEnv(graph=G8, k=3, render_mode=None)\n",
    "\n",
    "# 6.3) Create the GNN policy (actor‐critic)\n",
    "policy = GNNCopPolicy(in_feats=2, hidden_dim=64, emb_dim=64)\n",
    "policy = policy.to(\"cuda\")  # or \"cpu\" if no GPU\n",
    "\n",
    "# 6.4) Train with REINFORCE\n",
    "trained_policy = train_reinforce(env=env, \n",
    "                                 policy=policy, \n",
    "                                 num_episodes=3000, \n",
    "                                 gamma=0.99, \n",
    "                                 lr=1e-3)\n",
    "\n",
    "# 6.5) Evaluate 5 episodes\n",
    "for _ in range(5):\n",
    "    data, info = env.reset()\n",
    "    cop_positions = env.cop_pos\n",
    "    done = False\n",
    "    total_r = 0.0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            logits_list, _ = trained_policy(data.to(\"cuda\"), cop_positions)\n",
    "            # pick greedy actions for eval\n",
    "            actions = []\n",
    "            for logits in logits_list:\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                a_i = torch.argmax(probs).item()\n",
    "                actions.append(a_i)\n",
    "\n",
    "            # Convert to (i,j) moves\n",
    "            chosen_moves = []\n",
    "            num_nodes = data.x.size(0)\n",
    "            board_n = int(math.sqrt(num_nodes))\n",
    "            for i, a_i in enumerate(actions):\n",
    "                (ci, cj) = cop_positions[i]\n",
    "                v_idx = ci * board_n + cj\n",
    "                mask_src = (data.edge_index[0] == v_idx)\n",
    "                nbr_idxs = data.edge_index[1, mask_src].unique()\n",
    "\n",
    "                if a_i == 0:\n",
    "                    new_idx = v_idx\n",
    "                else:\n",
    "                    new_idx = int(nbr_idxs[a_i - 1])\n",
    "                new_i = new_idx // board_n\n",
    "                new_j = new_idx % board_n\n",
    "                chosen_moves.append((new_i, new_j))\n",
    "\n",
    "        # Step environment\n",
    "        data, r, terminated, truncated, info = env.step(chosen_moves)\n",
    "        total_r += r\n",
    "        cop_positions = env.cop_pos\n",
    "        done = terminated or truncated\n",
    "\n",
    "    print(\"Eval ep reward:\", total_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e095b64",
   "metadata": {},
   "source": [
    "BELOW IS PREVIOUS CODE FOR NON-GNN RL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e04981-9f9b-490b-b4f5-695f58d0f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4389.56929242651\n",
      "[(2, 0), (1, 1), (1, 1), (2, 1), (3, 0), (0, 0), (0, 4), (4, 0), (4, 5), (0, 1), (0, 1), (0, 2), (0, 2), (3, 5), (5, 3), (5, 0), (4, 0), (5, 0), (5, 3), (3, 1), (0, 1), (3, 1), (3, 4), (4, 3), (4, 3), (4, 3)]\n",
      "[[(1, 2), (5, 1), (1, 5)], [(4, 5), (4, 2), (4, 5)], [(4, 2), (5, 3), (3, 4)], [(4, 2), (4, 4), (3, 5)], [(4, 5), (1, 4), (2, 4)], [(4, 1), (5, 4), (3, 5)], [(3, 0), (3, 2), (4, 5)], [(3, 2), (3, 3), (3, 4)], [(1, 0), (3, 2), (2, 4)], [(1, 3), (4, 3), (2, 5)], [(5, 3), (4, 3), (2, 5)], [(2, 3), (3, 4), (2, 5)], [(2, 3), (3, 0), (3, 4)], [(5, 0), (5, 0), (0, 4)], [(4, 0), (0, 5), (4, 0)], [(3, 1), (0, 1), (4, 4)], [(2, 1), (0, 3), (5, 4)], [(0, 3), (4, 3), (3, 4)], [(1, 4), (1, 0), (3, 4)], [(2, 3), (1, 0), (5, 4)], [(1, 4), (3, 2), (5, 0)], [(2, 4), (2, 3), (5, 5)], [(5, 1), (4, 1), (5, 3)], [(5, 1), (3, 0), (2, 0)], [(5, 5), (3, 0), (5, 3)], [(5, 5), (3, 0), (5, 3)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/envs/registration.py:736: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='human' that is not in the possible render_modes ([]).\u001b[0m\n",
      "  logger.warn(\n",
      "/home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'sage.rings.real_mpfr.RealNumber'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3, render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "total_reward = 0.0\n",
    "c_states = list()\n",
    "r_states = list()\n",
    "\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample() # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "    cop_state, robber_state = env.unwrapped.get_moves()\n",
    "    #print(c_states)\n",
    "    c_states.append(copy.deepcopy(cop_state))\n",
    "    r_states.append(copy.deepcopy(robber_state))\n",
    "\n",
    "print(total_reward)\n",
    "print(r_states)\n",
    "print(c_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "611f3d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27399316e0b41658e090bdfbc8dbe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, max=25)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91b2c0525744a06a91cc88344b52b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_game(G, c_states, r_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21d291f3-828f-47b8-b524-19350a49039a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from gymnasium.spaces import Tuple, MultiDiscrete\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # force CPU\n",
    "\n",
    "class TupleToMultiDiscreteWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, Tuple)\n",
    "        self.original_space = env.action_space\n",
    "        self.action_space = MultiDiscrete([space.n for space in env.action_space])\n",
    "\n",
    "    def action(self, action):\n",
    "        return tuple(action)\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        return list(action)\n",
    "\n",
    "def make_env(G):\n",
    "    def _init():\n",
    "        env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3)\n",
    "        env = TupleToMultiDiscreteWrapper(env)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6a36436-91c3-4763-9ee4-86394a49930b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 258       |\n",
      "|    ep_rew_mean     | -2.04e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 21        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 776       |\n",
      "|    total_timesteps | 16384     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.04e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1563         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.934802e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | -3.05e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.06e+05     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00335     |\n",
      "|    value_loss           | 1.47e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.04e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2348         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.080705e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.86e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00304     |\n",
      "|    value_loss           | 1.46e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.04e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 20            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 3125          |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.8545305e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.99e+05      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00298      |\n",
      "|    value_loss           | 1.45e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.04e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 3876          |\n",
      "|    total_timesteps      | 81920         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.8803026e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.72e+05      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00304      |\n",
      "|    value_loss           | 1.43e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | -2.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 4646         |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.835225e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.82e+05     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    value_loss           | 1.41e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.04e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 5424         |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.272524e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.5e+05      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 1.4e+06      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.04e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 6201         |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.238864e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.58e+05     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    value_loss           | 1.37e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.04e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 6973         |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.948099e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.51e+05     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00306     |\n",
      "|    value_loss           | 1.36e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.04e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 7736         |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.589172e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.49e+05     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00316     |\n",
      "|    value_loss           | 1.34e+06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 256         |\n",
      "|    ep_rew_mean          | -2.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 8527        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.66659e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -12.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.42e+05    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00309    |\n",
      "|    value_loss           | 1.32e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | -2.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 9306         |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.620535e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.31e+05     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    value_loss           | 1.31e+06     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 258            |\n",
      "|    ep_rew_mean          | -2.04e+04      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 21             |\n",
      "|    iterations           | 13             |\n",
      "|    time_elapsed         | 10064          |\n",
      "|    total_timesteps      | 212992         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000101926926 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -12.3          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 6.05e+05       |\n",
      "|    n_updates            | 120            |\n",
      "|    policy_gradient_loss | -0.00318       |\n",
      "|    value_loss           | 1.28e+06       |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 258            |\n",
      "|    ep_rew_mean          | -2.04e+04      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 21             |\n",
      "|    iterations           | 14             |\n",
      "|    time_elapsed         | 10835          |\n",
      "|    total_timesteps      | 229376         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000100603036 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -12.3          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 6.18e+05       |\n",
      "|    n_updates            | 130            |\n",
      "|    policy_gradient_loss | -0.00309       |\n",
      "|    value_loss           | 1.26e+06       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.04e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 11608         |\n",
      "|    total_timesteps      | 245760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010740698 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.17e+05      |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.00321      |\n",
      "|    value_loss           | 1.25e+06      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 258            |\n",
      "|    ep_rew_mean          | -2.03e+04      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 21             |\n",
      "|    iterations           | 16             |\n",
      "|    time_elapsed         | 12382          |\n",
      "|    total_timesteps      | 262144         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000116857205 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -12.3          |\n",
      "|    explained_variance   | 2.38e-07       |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 5.84e+05       |\n",
      "|    n_updates            | 150            |\n",
      "|    policy_gradient_loss | -0.00329       |\n",
      "|    value_loss           | 1.23e+06       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 13094         |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014614334 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.83e+05      |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.00356      |\n",
      "|    value_loss           | 1.21e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.03e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 13882        |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001244789 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.69e+05     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00336     |\n",
      "|    value_loss           | 1.19e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.04e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 14646         |\n",
      "|    total_timesteps      | 311296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011939855 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.68e+05      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.00331      |\n",
      "|    value_loss           | 1.17e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 15414         |\n",
      "|    total_timesteps      | 327680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012765104 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.59e+05      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.0034       |\n",
      "|    value_loss           | 1.16e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 16187         |\n",
      "|    total_timesteps      | 344064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013285765 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.45e+05      |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.00346      |\n",
      "|    value_loss           | 1.13e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | -2.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 16966        |\n",
      "|    total_timesteps      | 360448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001449177 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.31e+05     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0035      |\n",
      "|    value_loss           | 1.12e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.04e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 17744        |\n",
      "|    total_timesteps      | 376832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001338143 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.38e+05     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00329     |\n",
      "|    value_loss           | 1.12e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 18513         |\n",
      "|    total_timesteps      | 393216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016169538 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.18e+05      |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.00364      |\n",
      "|    value_loss           | 1.09e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 19284         |\n",
      "|    total_timesteps      | 409600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017851347 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.92e+05      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.00373      |\n",
      "|    value_loss           | 1.07e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 20050         |\n",
      "|    total_timesteps      | 425984        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016732764 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.98e+05      |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.0037       |\n",
      "|    value_loss           | 1.05e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 20828         |\n",
      "|    total_timesteps      | 442368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019405971 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.77e+05      |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00376      |\n",
      "|    value_loss           | 1.03e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 21584         |\n",
      "|    total_timesteps      | 458752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017849538 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.85e+05      |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.00373      |\n",
      "|    value_loss           | 1.02e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 22349         |\n",
      "|    total_timesteps      | 475136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020007414 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.86e+05      |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.00377      |\n",
      "|    value_loss           | 1.01e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.03e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 23125        |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002341836 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.81e+05     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00395     |\n",
      "|    value_loss           | 9.95e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 23883         |\n",
      "|    total_timesteps      | 507904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023984976 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.67e+05      |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.00389      |\n",
      "|    value_loss           | 9.74e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 258       |\n",
      "|    ep_rew_mean     | -2.03e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 21        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 756       |\n",
      "|    total_timesteps | 524288    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 22            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 1472          |\n",
      "|    total_timesteps      | 540672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025918774 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.46e+05      |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.00387      |\n",
      "|    value_loss           | 9.45e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 2247          |\n",
      "|    total_timesteps      | 557056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032356166 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.37e+05      |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.00424      |\n",
      "|    value_loss           | 9.25e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 3017          |\n",
      "|    total_timesteps      | 573440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033470907 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.25e+05      |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.00422      |\n",
      "|    value_loss           | 9e+05         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 3801          |\n",
      "|    total_timesteps      | 589824        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037888263 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.33e+05      |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.00436      |\n",
      "|    value_loss           | 9.01e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.03e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 4602         |\n",
      "|    total_timesteps      | 606208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004211767 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.2e+05      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    value_loss           | 8.77e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 5374          |\n",
      "|    total_timesteps      | 622592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042017412 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.16e+05      |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -0.00435      |\n",
      "|    value_loss           | 8.64e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 6149          |\n",
      "|    total_timesteps      | 638976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054473156 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.3         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.98e+05      |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00459      |\n",
      "|    value_loss           | 8.44e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 6914          |\n",
      "|    total_timesteps      | 655360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046729972 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.2         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.94e+05      |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.00439      |\n",
      "|    value_loss           | 8.32e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 7713         |\n",
      "|    total_timesteps      | 671744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005577512 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.9e+05      |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00467     |\n",
      "|    value_loss           | 8.17e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 258           |\n",
      "|    ep_rew_mean          | -2.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 8526          |\n",
      "|    total_timesteps      | 688128        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068662735 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -12.2         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.75e+05      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.00482      |\n",
      "|    value_loss           | 8.01e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 9321         |\n",
      "|    total_timesteps      | 704512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006803999 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.8e+05      |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00479     |\n",
      "|    value_loss           | 7.9e+05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 10075        |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008872665 |\n",
      "|    clip_fraction        | 5.49e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.64e+05     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00512     |\n",
      "|    value_loss           | 7.74e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 10814        |\n",
      "|    total_timesteps      | 737280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009837118 |\n",
      "|    clip_fraction        | 2.44e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.62e+05     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00528     |\n",
      "|    value_loss           | 7.61e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 11586        |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010160524 |\n",
      "|    clip_fraction        | 6.1e-06      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.29e+05     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00515     |\n",
      "|    value_loss           | 7.37e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 256         |\n",
      "|    ep_rew_mean          | -2e+04      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 12337       |\n",
      "|    total_timesteps      | 770048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001175132 |\n",
      "|    clip_fraction        | 0.000171    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -12.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.43e+05    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    value_loss           | 7.51e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 13097        |\n",
      "|    total_timesteps      | 786432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017061471 |\n",
      "|    clip_fraction        | 0.000208     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.24e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    value_loss           | 7.18e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -2.01e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 13815       |\n",
      "|    total_timesteps      | 802816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001896184 |\n",
      "|    clip_fraction        | 0.00047     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -12.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.45e+05    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    value_loss           | 7.03e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 14578        |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020262774 |\n",
      "|    clip_fraction        | 0.000708     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.19e+05     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00627     |\n",
      "|    value_loss           | 6.82e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 15337        |\n",
      "|    total_timesteps      | 835584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022122003 |\n",
      "|    clip_fraction        | 0.000745     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.17e+05     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00632     |\n",
      "|    value_loss           | 6.71e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 16109        |\n",
      "|    total_timesteps      | 851968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025370768 |\n",
      "|    clip_fraction        | 0.00141      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.1        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.11e+05     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00641     |\n",
      "|    value_loss           | 6.57e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 16861        |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025086538 |\n",
      "|    clip_fraction        | 0.00126      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.1        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.05e+05     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00641     |\n",
      "|    value_loss           | 6.49e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 17626        |\n",
      "|    total_timesteps      | 884736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024274806 |\n",
      "|    clip_fraction        | 0.00134      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.1        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98e+05     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00627     |\n",
      "|    value_loss           | 6.29e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 18381        |\n",
      "|    total_timesteps      | 901120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027819579 |\n",
      "|    clip_fraction        | 0.0027       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.1        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.86e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00668     |\n",
      "|    value_loss           | 6.18e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 19143        |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033526057 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.1        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.91e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00722     |\n",
      "|    value_loss           | 6.09e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 19915        |\n",
      "|    total_timesteps      | 933888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037134495 |\n",
      "|    clip_fraction        | 0.00517      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12.1        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.82e+05     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00724     |\n",
      "|    value_loss           | 5.95e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 20702        |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034145662 |\n",
      "|    clip_fraction        | 0.00362      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12          |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.65e+05     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.0068      |\n",
      "|    value_loss           | 5.78e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 21472        |\n",
      "|    total_timesteps      | 966656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043394547 |\n",
      "|    clip_fraction        | 0.00701      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12          |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.6e+05      |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0078      |\n",
      "|    value_loss           | 5.67e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 22218        |\n",
      "|    total_timesteps      | 983040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043141944 |\n",
      "|    clip_fraction        | 0.00954      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12          |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.62e+05     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00796     |\n",
      "|    value_loss           | 5.56e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 22973        |\n",
      "|    total_timesteps      | 999424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044223233 |\n",
      "|    clip_fraction        | 0.00894      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12          |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.58e+05     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00754     |\n",
      "|    value_loss           | 5.48e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 23748        |\n",
      "|    total_timesteps      | 1015808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046329885 |\n",
      "|    clip_fraction        | 0.00895      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -12          |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.47e+05     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00724     |\n",
      "|    value_loss           | 5.32e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 258       |\n",
      "|    ep_rew_mean     | -2.01e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 22        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 726       |\n",
      "|    total_timesteps | 1032192   |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1479         |\n",
      "|    total_timesteps      | 1048576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053928494 |\n",
      "|    clip_fraction        | 0.0194       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.9        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.37e+05     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00878     |\n",
      "|    value_loss           | 5.11e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2195         |\n",
      "|    total_timesteps      | 1064960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055353334 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.9        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39e+05     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00848     |\n",
      "|    value_loss           | 4.98e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2951         |\n",
      "|    total_timesteps      | 1081344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053971624 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.8        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.23e+05     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.00807     |\n",
      "|    value_loss           | 4.78e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3712         |\n",
      "|    total_timesteps      | 1097728      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058649974 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.8        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.28e+05     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00822     |\n",
      "|    value_loss           | 4.73e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -2e+04      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4492        |\n",
      "|    total_timesteps      | 1114112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006030294 |\n",
      "|    clip_fraction        | 0.0216      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.8       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.1e+05     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00864    |\n",
      "|    value_loss           | 4.65e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 257          |\n",
      "|    ep_rew_mean          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 5263         |\n",
      "|    total_timesteps      | 1130496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057602488 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.8        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.13e+05     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00857     |\n",
      "|    value_loss           | 4.48e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 257         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 6040        |\n",
      "|    total_timesteps      | 1146880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005546245 |\n",
      "|    clip_fraction        | 0.0187      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.08e+05    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00752    |\n",
      "|    value_loss           | 4.75e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -2e+04      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 6824        |\n",
      "|    total_timesteps      | 1163264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005508312 |\n",
      "|    clip_fraction        | 0.0209      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.99e+05    |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00835    |\n",
      "|    value_loss           | 4.29e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 7605         |\n",
      "|    total_timesteps      | 1179648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060393615 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.8        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.91e+05     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00846     |\n",
      "|    value_loss           | 4.23e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 8365         |\n",
      "|    total_timesteps      | 1196032      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057167336 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.8        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97e+05     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0081      |\n",
      "|    value_loss           | 4.12e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 257          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 9145         |\n",
      "|    total_timesteps      | 1212416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060973912 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.8        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97e+05     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00892     |\n",
      "|    value_loss           | 3.99e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 9912         |\n",
      "|    total_timesteps      | 1228800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064165248 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.8        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.24e+05     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.00874     |\n",
      "|    value_loss           | 4.27e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 10656        |\n",
      "|    total_timesteps      | 1245184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058466475 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+05     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00875     |\n",
      "|    value_loss           | 3.83e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 11397        |\n",
      "|    total_timesteps      | 1261568      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062296903 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.62e+05     |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00895     |\n",
      "|    value_loss           | 3.68e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 12145        |\n",
      "|    total_timesteps      | 1277952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061008874 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52e+05     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.0088      |\n",
      "|    value_loss           | 3.65e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 12909        |\n",
      "|    total_timesteps      | 1294336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065163556 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.59e+05     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00926     |\n",
      "|    value_loss           | 3.48e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 13675        |\n",
      "|    total_timesteps      | 1310720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064475494 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.61e+05     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00856     |\n",
      "|    value_loss           | 3.43e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -2e+04      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 14435       |\n",
      "|    total_timesteps      | 1327104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006325639 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.47e+05    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    value_loss           | 3.38e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 15207        |\n",
      "|    total_timesteps      | 1343488      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067147743 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.6        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.47e+05     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00978     |\n",
      "|    value_loss           | 3.23e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 15981        |\n",
      "|    total_timesteps      | 1359872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061713858 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.6        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.4e+05      |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00827     |\n",
      "|    value_loss           | 3.14e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 16737        |\n",
      "|    total_timesteps      | 1376256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065355147 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.6        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+05     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00929     |\n",
      "|    value_loss           | 3.06e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -2e+04      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 17495       |\n",
      "|    total_timesteps      | 1392640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006396077 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.36e+05    |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00887    |\n",
      "|    value_loss           | 2.98e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 18248        |\n",
      "|    total_timesteps      | 1409024      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067962487 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.6        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+05     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00971     |\n",
      "|    value_loss           | 2.92e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 19021        |\n",
      "|    total_timesteps      | 1425408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063266447 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.6        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.27e+05     |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.00896     |\n",
      "|    value_loss           | 2.83e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | -1.97e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 19804        |\n",
      "|    total_timesteps      | 1441792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070046177 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.6        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33e+05     |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.00972     |\n",
      "|    value_loss           | 2.76e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | -1.97e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 20544        |\n",
      "|    total_timesteps      | 1458176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062487265 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.5        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+05     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00881     |\n",
      "|    value_loss           | 3.03e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -2e+04       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 21310        |\n",
      "|    total_timesteps      | 1474560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066676643 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.5        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+05      |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00959     |\n",
      "|    value_loss           | 2.6e+05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 22068        |\n",
      "|    total_timesteps      | 1490944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067601693 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.5        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.03e+05     |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00966     |\n",
      "|    value_loss           | 2.53e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 22814        |\n",
      "|    total_timesteps      | 1507328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070751156 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.5        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.16e+05     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    value_loss           | 2.43e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 258        |\n",
      "|    ep_rew_mean          | -1.99e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 23591      |\n",
      "|    total_timesteps      | 1523712    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00732121 |\n",
      "|    clip_fraction        | 0.0544     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -11.4      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.83e+04   |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    value_loss           | 2.36e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 258       |\n",
      "|    ep_rew_mean     | -1.99e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 22        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 727       |\n",
      "|    total_timesteps | 1540096   |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1499        |\n",
      "|    total_timesteps      | 1556480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007104248 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.88e+04    |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 2.22e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2259         |\n",
      "|    total_timesteps      | 1572864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070449933 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.05e+04     |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.00994     |\n",
      "|    value_loss           | 2.15e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 3012         |\n",
      "|    total_timesteps      | 1589248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067383163 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.96e+04     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00965     |\n",
      "|    value_loss           | 2.06e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 3784        |\n",
      "|    total_timesteps      | 1605632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007450819 |\n",
      "|    clip_fraction        | 0.0547      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.03e+04    |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 2.02e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4553        |\n",
      "|    total_timesteps      | 1622016     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006999355 |\n",
      "|    clip_fraction        | 0.0518      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.26e+04    |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 1.95e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 5318        |\n",
      "|    total_timesteps      | 1638400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006985775 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.58e+04    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 1.89e+05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39mInteger(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Integer(\u001b[38;5;241m10\u001b[39m)):\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500_000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_cr_16x16_itr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39mInteger(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#model.learn(total_timesteps=1_000)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#model.save(\"ppo_cr_16.zip\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:252\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m     entropy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mth\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39mlog_prob)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m     entropy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentropy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m entropy_losses\u001b[38;5;241m.\u001b[39mappend(entropy_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    256\u001b[0m loss \u001b[38;5;241m=\u001b[39m policy_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ment_coef \u001b[38;5;241m*\u001b[39m entropy_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf_coef \u001b[38;5;241m*\u001b[39m value_loss\n",
      "File \u001b[0;32msignals.pyx:355\u001b[0m, in \u001b[0;36mcysignals.signals.python_check_interrupt\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G = make_graph(16, queen, False)\n",
    "\n",
    "# Create vectorized environment with, say, 8 parallel copies\n",
    "env = make_vec_env(make_env(G), n_envs=8)\n",
    "#env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3, render_mode=\"human\")\n",
    "#env = TupleToMultiDiscreteWrapper(env)\n",
    "\n",
    "# Train PPO w/checkpointing\n",
    "model = PPO(\"MultiInputPolicy\", env, device='cuda', verbose=1)\n",
    "for i in range(10):\n",
    "    model.learn(total_timesteps=500_000, reset_num_timesteps=False)\n",
    "    model.save(f\"ppo_cr_16x16_itr{i+1}\")\n",
    "\n",
    "#model.learn(total_timesteps=1_000)\n",
    "#model.save(\"ppo_cr_16.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f4a7336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 258      |\n",
      "|    ep_rew_mean     | -2e+04   |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 744      |\n",
      "|    total_timesteps | 1654784  |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1515         |\n",
      "|    total_timesteps      | 1671168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064652245 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.14e+04     |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    value_loss           | 1.83e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2285         |\n",
      "|    total_timesteps      | 1687552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067456095 |\n",
      "|    clip_fraction        | 0.044        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.98e+04     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.00967     |\n",
      "|    value_loss           | 1.74e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 257         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3061        |\n",
      "|    total_timesteps      | 1703936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006816312 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.05e+04    |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.7e+05     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3846         |\n",
      "|    total_timesteps      | 1720320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071315244 |\n",
      "|    clip_fraction        | 0.0549       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.25e+05     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    value_loss           | 2.15e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4624        |\n",
      "|    total_timesteps      | 1736704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007181278 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7e+04       |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 1.58e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 5386         |\n",
      "|    total_timesteps      | 1753088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071011735 |\n",
      "|    clip_fraction        | 0.0546       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.14e+04     |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    value_loss           | 1.5e+05      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 6142        |\n",
      "|    total_timesteps      | 1769472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007534826 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.01e+04    |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 1.46e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 6899         |\n",
      "|    total_timesteps      | 1785856      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074004745 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.2        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.2e+04      |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 1.4e+05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 7673         |\n",
      "|    total_timesteps      | 1802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069772936 |\n",
      "|    clip_fraction        | 0.052        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.73e+04     |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    value_loss           | 1.35e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 8439         |\n",
      "|    total_timesteps      | 1818624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076920795 |\n",
      "|    clip_fraction        | 0.0588       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.1        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.39e+04     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.0115      |\n",
      "|    value_loss           | 1.32e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 9193         |\n",
      "|    total_timesteps      | 1835008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068772864 |\n",
      "|    clip_fraction        | 0.0488       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.1        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.03e+04     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    value_loss           | 1.26e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 9950         |\n",
      "|    total_timesteps      | 1851392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068102614 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.1        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.84e+04     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    value_loss           | 1.21e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 10712       |\n",
      "|    total_timesteps      | 1867776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006913298 |\n",
      "|    clip_fraction        | 0.0557      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.88e+04    |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 1.13e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 258        |\n",
      "|    ep_rew_mean          | -1.99e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 11492      |\n",
      "|    total_timesteps      | 1884160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00691266 |\n",
      "|    clip_fraction        | 0.0569     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -11.1      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.27e+04   |\n",
      "|    n_updates            | 1130       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 1.1e+05    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 12258       |\n",
      "|    total_timesteps      | 1900544     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006617681 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11         |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.72e+04    |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 1.06e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 13034       |\n",
      "|    total_timesteps      | 1916928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006972975 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.06e+04    |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 1.01e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 13794        |\n",
      "|    total_timesteps      | 1933312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072929384 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11          |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.23e+04     |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0124      |\n",
      "|    value_loss           | 9.62e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 258        |\n",
      "|    ep_rew_mean          | -1.99e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 14564      |\n",
      "|    total_timesteps      | 1949696    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00720746 |\n",
      "|    clip_fraction        | 0.0604     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -11.1      |\n",
      "|    explained_variance   | 5.96e-08   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.69e+04   |\n",
      "|    n_updates            | 1170       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    value_loss           | 9.23e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 15344       |\n",
      "|    total_timesteps      | 1966080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007034725 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.35e+04    |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 8.97e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 16114       |\n",
      "|    total_timesteps      | 1982464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007815565 |\n",
      "|    clip_fraction        | 0.0718      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11         |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.99e+04    |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 8.64e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 16893       |\n",
      "|    total_timesteps      | 1998848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007267709 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.88e+04    |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 7.93e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 257          |\n",
      "|    ep_rew_mean          | -1.97e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 17663        |\n",
      "|    total_timesteps      | 2015232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070968103 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10.9        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.57e+04     |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.0129      |\n",
      "|    value_loss           | 7.87e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 18429       |\n",
      "|    total_timesteps      | 2031616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007216166 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.57e+04    |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 1.36e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 19184        |\n",
      "|    total_timesteps      | 2048000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070150318 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10.9        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.15e+04     |\n",
      "|    n_updates            | 1230         |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 6.98e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 19942        |\n",
      "|    total_timesteps      | 2064384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074436213 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10.9        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99e+04     |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0142      |\n",
      "|    value_loss           | 6.8e+04      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 20707       |\n",
      "|    total_timesteps      | 2080768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007298394 |\n",
      "|    clip_fraction        | 0.0629      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.46e+04    |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 6.5e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 21476        |\n",
      "|    total_timesteps      | 2097152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074086958 |\n",
      "|    clip_fraction        | 0.0692       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10.9        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.08e+04     |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.0141      |\n",
      "|    value_loss           | 5.99e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 22247        |\n",
      "|    total_timesteps      | 2113536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071697505 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10.9        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.12e+04     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.0146      |\n",
      "|    value_loss           | 5.82e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 22995       |\n",
      "|    total_timesteps      | 2129920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007630349 |\n",
      "|    clip_fraction        | 0.0701      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.9       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.81e+04    |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 5.69e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 23762        |\n",
      "|    total_timesteps      | 2146304      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074294466 |\n",
      "|    clip_fraction        | 0.0725       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10.9        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.95e+04     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.0171      |\n",
      "|    value_loss           | 5.33e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 258        |\n",
      "|    ep_rew_mean          | -1.98e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 24527      |\n",
      "|    total_timesteps      | 2162688    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00734337 |\n",
      "|    clip_fraction        | 0.0662     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.8      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.97e+04   |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    value_loss           | 5.07e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 25294       |\n",
      "|    total_timesteps      | 2179072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007816643 |\n",
      "|    clip_fraction        | 0.0707      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.11e+04    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 4.84e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 26056       |\n",
      "|    total_timesteps      | 2195456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007639668 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.41e+04    |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 4.64e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.99e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 26826        |\n",
      "|    total_timesteps      | 2211840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072970847 |\n",
      "|    clip_fraction        | 0.0665       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10.8        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.29e+04     |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | -0.0175      |\n",
      "|    value_loss           | 4.57e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 27588        |\n",
      "|    total_timesteps      | 2228224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074200677 |\n",
      "|    clip_fraction        | 0.0721       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -10.8        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+04     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0188      |\n",
      "|    value_loss           | 4.37e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 28348       |\n",
      "|    total_timesteps      | 2244608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008033969 |\n",
      "|    clip_fraction        | 0.0789      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.13e+04    |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 3.89e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 29120       |\n",
      "|    total_timesteps      | 2260992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008369586 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77e+04    |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 3.97e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 29895       |\n",
      "|    total_timesteps      | 2277376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008622082 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.85e+04    |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 3.69e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 30656       |\n",
      "|    total_timesteps      | 2293760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008884812 |\n",
      "|    clip_fraction        | 0.0822      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.91e+04    |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 3.6e+04     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 258        |\n",
      "|    ep_rew_mean          | -1.98e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 31419      |\n",
      "|    total_timesteps      | 2310144    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00852199 |\n",
      "|    clip_fraction        | 0.0837     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.8      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.7e+04    |\n",
      "|    n_updates            | 1390       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    value_loss           | 3.39e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 32186       |\n",
      "|    total_timesteps      | 2326528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009216446 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77e+04    |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 3.27e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 32991       |\n",
      "|    total_timesteps      | 2342912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009682195 |\n",
      "|    clip_fraction        | 0.0883      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.22e+03    |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 3.26e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 33811       |\n",
      "|    total_timesteps      | 2359296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019424435 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.89e+03    |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 3.1e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 34643       |\n",
      "|    total_timesteps      | 2375680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010568045 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.74e+04    |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 3.03e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 35441       |\n",
      "|    total_timesteps      | 2392064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019357534 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.36e+04    |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 2.98e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 258        |\n",
      "|    ep_rew_mean          | -1.98e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 36260      |\n",
      "|    total_timesteps      | 2408448    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02940626 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.7      |\n",
      "|    explained_variance   | 5.96e-08   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.6e+03    |\n",
      "|    n_updates            | 1450       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    value_loss           | 2.9e+04    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.99e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 37076       |\n",
      "|    total_timesteps      | 2424832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029171113 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.7       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.6e+03     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 3.04e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 258        |\n",
      "|    ep_rew_mean          | -1.98e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 37887      |\n",
      "|    total_timesteps      | 2441216    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02592903 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.7      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.69e+04   |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    value_loss           | 2.96e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 38639       |\n",
      "|    total_timesteps      | 2457600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027670525 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.59e+03    |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 1.11e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 39375       |\n",
      "|    total_timesteps      | 2473984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023797663 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.27e+04    |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.00938    |\n",
      "|    value_loss           | 2.89e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 258        |\n",
      "|    ep_rew_mean          | -1.98e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 40137      |\n",
      "|    total_timesteps      | 2490368    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02208788 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.7      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.62e+04   |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    value_loss           | 2.9e+04    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 258         |\n",
      "|    ep_rew_mean          | -1.98e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 40962       |\n",
      "|    total_timesteps      | 2506752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021342795 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -10.7       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.93e+04    |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 2.88e+04    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m make_vec_env(make_env(G), n_envs\u001b[38;5;241m=\u001b[39mInteger(\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2_000_000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_cr16.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 324\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:71\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m---> 71\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/core.py:333\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/wrappers/common.py:400\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/core.py:333\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/wrappers/common.py:295\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, seed\u001b[38;5;241m=\u001b[39mseed, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 95\u001b[0m, in \u001b[0;36mCopsAndRobbersEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcop_pos \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobber_pos \u001b[38;5;241m=\u001b[39m \u001b[43mmaximize_available\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcop_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#pick maximizing available start for robber\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitr \u001b[38;5;241m=\u001b[39m Integer(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Return initial state\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m, in \u001b[0;36mmaximize_available\u001b[0;34m(cop_pos, cop_states, robber_states, robber_pos)\u001b[0m\n\u001b[1;32m    196\u001b[0m moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m() \u001b[38;5;66;03m# move -> min cop move in anticipation\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m move \u001b[38;5;129;01min\u001b[39;00m r_neighbors:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m#Q: here, do we want cop moves to take into account that cops wont repeat moves ?\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m#at this point, cops are making suboptimal moves\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# i guess dont take into account, as robber doesnt care for repeating moves?\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     cop_response \u001b[38;5;241m=\u001b[39m \u001b[43mminimize_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcop_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     max_min_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(available_squares(cop_response, move))\n\u001b[1;32m    205\u001b[0m     moves[move] \u001b[38;5;241m=\u001b[39m max_min_val\n",
      "Cell \u001b[0;32mIn[1], line 183\u001b[0m, in \u001b[0;36mminimize_available\u001b[0;34m(cop_pos, robber_pos, cop_states, robber_states)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mminimize_available\u001b[39m(cop_pos: \u001b[38;5;28mlist\u001b[39m, robber_pos: \u001b[38;5;28mtuple\u001b[39m, cop_states, robber_states) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Function that returns the move for the cops that minimizes the number of available squares for the robber\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m#this function could be the combinatorially large one, but we are going to introduce our greedy heuristic\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m#filter out whichever are on occupied axes\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m#use backtracking algorithm, recursively call min_avail_helper w/i+1, new cop_pos\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     occupied_axes \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegd\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposd\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     }\n\u001b[0;32m--> 183\u001b[0m     min_config, min_squares \u001b[38;5;241m=\u001b[39m \u001b[43mminimize_avail_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcop_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobber_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moccupied_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcop_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobber_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m min_config\n",
      "Cell \u001b[0;32mIn[1], line 145\u001b[0m, in \u001b[0;36mminimize_avail_helper\u001b[0;34m(curr_cop_pos, robber_pos, i, occupied_axes, cop_moves, robber_moves)\u001b[0m\n\u001b[1;32m    142\u001b[0m new_cop_pos \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(curr_cop_pos)\n\u001b[1;32m    143\u001b[0m new_cop_pos[i] \u001b[38;5;241m=\u001b[39m move \u001b[38;5;66;03m#move cop i\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m curr_config, curr_squares \u001b[38;5;241m=\u001b[39m \u001b[43mminimize_avail_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_cop_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobber_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moccupied_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcop_moves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobber_moves\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m moves\u001b[38;5;241m.\u001b[39mappend((curr_config, curr_squares))\n\u001b[1;32m    148\u001b[0m occupied_axes[axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 145\u001b[0m, in \u001b[0;36mminimize_avail_helper\u001b[0;34m(curr_cop_pos, robber_pos, i, occupied_axes, cop_moves, robber_moves)\u001b[0m\n\u001b[1;32m    142\u001b[0m new_cop_pos \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(curr_cop_pos)\n\u001b[1;32m    143\u001b[0m new_cop_pos[i] \u001b[38;5;241m=\u001b[39m move \u001b[38;5;66;03m#move cop i\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m curr_config, curr_squares \u001b[38;5;241m=\u001b[39m \u001b[43mminimize_avail_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_cop_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobber_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moccupied_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcop_moves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobber_moves\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m moves\u001b[38;5;241m.\u001b[39mappend((curr_config, curr_squares))\n\u001b[1;32m    148\u001b[0m occupied_axes[axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 164\u001b[0m, in \u001b[0;36mminimize_avail_helper\u001b[0;34m(curr_cop_pos, robber_pos, i, occupied_axes, cop_moves, robber_moves)\u001b[0m\n\u001b[1;32m    161\u001b[0m min_avail_moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m tup: tup[Integer(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m==\u001b[39m min_val, moves))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m#sort by which config gives the max min SD\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m best_move \u001b[38;5;241m=\u001b[39m \u001b[43mget_min_LSD_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrobber_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_avail_moves\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_move[Integer(\u001b[38;5;241m0\u001b[39m)], best_move[Integer(\u001b[38;5;241m1\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m, in \u001b[0;36mget_min_LSD_move\u001b[0;34m(robber_pos, moves)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#get LSD of robber for this possible move\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r_move \u001b[38;5;129;01min\u001b[39;00m r_moves:\n\u001b[0;32m---> 77\u001b[0m     SD_len \u001b[38;5;241m=\u001b[39m \u001b[43mget_SD_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_move\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SD_len \u001b[38;5;241m>\u001b[39m LSD:\n\u001b[1;32m     79\u001b[0m         LSD \u001b[38;5;241m=\u001b[39m SD_len\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mget_SD_length\u001b[0;34m(robber_pos)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m robber_pos[Integer(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m+\u001b[39m robber_pos[Integer(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     62\u001b[0m posd_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m posn: posn[Integer(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m-\u001b[39m posn[Integer(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m==\u001b[39m diff, G\u001b[38;5;241m.\u001b[39mvertices())))\n\u001b[0;32m---> 63\u001b[0m negd_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m posn: posn[Integer(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m+\u001b[39m posn[Integer(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msum\u001b[39m, G\u001b[38;5;241m.\u001b[39mvertices())))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(posd_len, negd_len)\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mget_SD_length.<locals>.<lambda>\u001b[0;34m(posn)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m robber_pos[Integer(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m+\u001b[39m robber_pos[Integer(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     62\u001b[0m posd_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m posn: posn[Integer(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m-\u001b[39m posn[Integer(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m==\u001b[39m diff, G\u001b[38;5;241m.\u001b[39mvertices())))\n\u001b[0;32m---> 63\u001b[0m negd_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m posn: posn[Integer(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m+\u001b[39m posn[\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msum\u001b[39m, G\u001b[38;5;241m.\u001b[39mvertices())))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(posd_len, negd_len)\n",
      "File \u001b[0;32msignals.pyx:355\u001b[0m, in \u001b[0;36mcysignals.signals.python_check_interrupt\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = make_vec_env(make_env(G), n_envs=8)\n",
    "model.learn(total_timesteps=2_000_000, reset_num_timesteps=False)\n",
    "\n",
    "model.save(\"ppo_cr16.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "447d82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cr16.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a97aa0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (1.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.2.6)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.7.0)\n",
      "Requirement already satisfied: cloudpickle in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: pandas in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (3.10.3)\n",
      "Requirement already satisfied: opencv-python in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (4.11.0.86)\n",
      "Requirement already satisfied: pygame in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.19.0)\n",
      "Requirement already satisfied: psutil in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (4.67.1)\n",
      "Requirement already satisfied: rich in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (14.0.0)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: pillow in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (11.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: filelock in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from triton==3.3.0->torch<3.0,>=2.3->stable-baselines3[extra]) (80.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.8)\n",
      "Requirement already satisfied: packaging in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (6.31.0)\n",
      "Requirement already satisfied: six>1.9 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90adf1cf-964b-4701-82e3-1e061f98f8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: -20927.9166402739, number of rounds: 258\n",
      "Mean reward: -20711.09934387207 ± 180.3704003720745\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "#model = PPO.load('/mnt/c/Users/danie/reu2025/reu2025/ppo_cr_v1', device='cpu')\n",
    "env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3)\n",
    "env = TupleToMultiDiscreteWrapper(env)\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "c_states = list()\n",
    "r_states = list()\n",
    "\n",
    "G = make_graph(16, queen, False)\n",
    "\n",
    "#env = make_vec_env(make_env(G), n_envs=8)\n",
    "obs, _ = env.reset()\n",
    "#cop_state, robber_state = env.envs[0].unwrapped.get_moves()\n",
    "cop_state, robber_state = env.unwrapped.get_moves()\n",
    "c_states.append(copy.deepcopy(cop_state))\n",
    "r_states.append(copy.deepcopy(robber_state))\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "itr = 0\n",
    "\n",
    "while not done:\n",
    "    itr += 1\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    '''\n",
    "    obs, rewards, dones, infos = env.step(action)\n",
    "    total_reward += rewards[0]      # since vectorized you get a 1‑element batch\n",
    "    done = dones[0]\n",
    "    print(dones)\n",
    "    '''\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    total_reward += rewards\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done: #lmao\n",
    "        break\n",
    "\n",
    "    cop_state, robber_state = env.unwrapped.get_moves()\n",
    "    c_states.append(copy.deepcopy(cop_state))\n",
    "    r_states.append(r_states[-1])\n",
    "\n",
    "    c_states.append(c_states[-1])\n",
    "    r_states.append(copy.deepcopy(robber_state))\n",
    "\n",
    "print(f\"total reward: {total_reward}, number of rounds: {itr}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1)\n",
    "print(f\"Mean reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "882df629-e63d-48a7-9df6-667deca2921b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049e43324efb4568a0f50788ce6c9826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, max=514)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cc0bb5b06e4b4b94455632d28e4e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_game(G, c_states, r_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.6",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
