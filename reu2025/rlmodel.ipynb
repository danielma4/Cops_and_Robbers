{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3caa9672",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NOTES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8267ec-ba69-4e2c-a90e-d49bb3052348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.all import graphs\n",
    "import copy, sys, math\n",
    "\n",
    "\n",
    "def available_squares(cop_pos: list, robber_pos: tuple) -> list:\n",
    "    #Function that returns the set of squares that are available to the robber\n",
    "    #return the adjacent vertices of robber_pos \\ adjacent vertices of cops    \n",
    "    c_neighbors = set(cop_pos)\n",
    "    for cop in cop_pos:\n",
    "        c_neighbors = c_neighbors.union(set(G.neighbors(cop)))\n",
    "\n",
    "    if robber_pos == (-1, -1): \n",
    "        return set(G.vertices()) - c_neighbors\n",
    "    else:\n",
    "        r_neighbors = set(G.neighbors(robber_pos)).union({robber_pos})\n",
    "        return r_neighbors - c_neighbors\n",
    "\n",
    "def get_axis(cop_pos, robber_pos):\n",
    "    #returns 'h', 'v', 'posd', 'negd' or none for axis which a cop position occupies\n",
    "    #assuming given correct input ie intersecting but not same posn\n",
    "    if cop_pos == robber_pos:\n",
    "        return 'h'\n",
    "        \n",
    "    if robber_pos[0] == cop_pos[0]:\n",
    "        return 'v'\n",
    "    elif robber_pos[1] == cop_pos[1]:\n",
    "        return 'h'\n",
    "    elif cop_pos[0] + cop_pos[1] == robber_pos[0] + robber_pos[1]:\n",
    "        return 'negd'\n",
    "    elif cop_pos[0] - cop_pos[1] == robber_pos[0] - robber_pos[1]:\n",
    "        return 'posd'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_axes_squares(robber_pos, avail, occupied_axes):\n",
    "    #filters out the squares which are on a cop occupied axis from a list of vertices\n",
    "    for axis in map(lambda a: a[0], set(filter(lambda a: a[1], occupied_axes.items()))):\n",
    "        if axis == 'h':\n",
    "            #remove all with move[1] == robber_pos[1]\n",
    "            avail = set(filter(lambda posn: posn[1] != robber_pos[1], avail))\n",
    "        elif axis == 'v':\n",
    "            #remove all with move[0] == robber_pos[0]\n",
    "            avail = set(filter(lambda posn: posn[0] != robber_pos[0], avail))\n",
    "        elif axis == 'negd':\n",
    "            #remove all w/move[0]+move[1] == robber_pos[0]+robber_pos[1]\n",
    "            avail = set(filter(lambda posn: posn[0] + posn[1] != robber_pos[0] + robber_pos[1], avail))\n",
    "        elif axis == 'posd':\n",
    "            #remove all w/move[0]-move[1] == robber_pos[0]-robber_pos[1]\n",
    "            avail = set(filter(lambda posn: posn[0] - posn[1] != robber_pos[0] - robber_pos[1], avail))\n",
    "    return avail\n",
    "\n",
    "def get_intersecting_squares(cop_pos, robber_pos, occupied_axes):\n",
    "    #compiles a list of the squares which cops can reach that directly attack the robber on a unique line\n",
    "    avail = set(G.neighbors(cop_pos)).union({cop_pos}).intersection(\n",
    "        set(G.neighbors(robber_pos)).union({robber_pos}))\n",
    "    return remove_axes_squares(robber_pos, avail, occupied_axes)\n",
    "\n",
    "def get_SD_length(robber_pos):\n",
    "    #determines the length of the SD\n",
    "    diff = robber_pos[0] - robber_pos[1]\n",
    "    sum = robber_pos[0] + robber_pos[1]\n",
    "    posd_len = len(list(filter(lambda posn: posn[0] - posn[1] == diff, G.vertices())))\n",
    "    negd_len = len(list(filter(lambda posn: posn[0] + posn[1] == sum, G.vertices())))\n",
    "    return min(posd_len, negd_len)\n",
    "\n",
    "def get_min_LSD_move(robber_pos: tuple, moves: list) -> tuple:\n",
    "    #determines the cop config which forces the minimum LSD\n",
    "    min_LSD = sys.maxsize\n",
    "    min_move = tuple()\n",
    "\n",
    "    for move in moves:\n",
    "        r_moves = available_squares(move[0], robber_pos)\n",
    "        LSD = -1\n",
    "\n",
    "        #get LSD of robber for this possible move\n",
    "        for r_move in r_moves:\n",
    "            SD_len = get_SD_length(r_move)\n",
    "            if SD_len > LSD:\n",
    "                LSD = SD_len\n",
    "    \n",
    "    if LSD < min_LSD:\n",
    "        min_LSD = LSD\n",
    "        min_move = move\n",
    "    \n",
    "    return min_move\n",
    "\n",
    "def seenp(move: list, robber_pos: tuple, cop_states: list, robber_states: list)-> bool:\n",
    "    if not (cop_states and robber_states): #empty states given for some reason\n",
    "        return False\n",
    "    \n",
    "    for t in range(len(cop_states) - 1, -1, -1):\n",
    "        if (set(cop_states[t]) == set(move) and robber_states[t] == robber_pos):\n",
    "            print(\"SEEN\")\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "#n^2 algo where n = # available moves, which is constant...\n",
    "def remove_seen_moves(moves: list, robber_pos: tuple, cop_states: list, robber_states: list)-> list:\n",
    "    #removes cop moves from a list of available cop moves if that state has already been seen\n",
    "\n",
    "    if (len(cop_states) != len(robber_states)):\n",
    "        raise Exception(\"given nonmatching states\")\n",
    "    \n",
    "\n",
    "    new_moves = [(m,v) for (m,v) in moves\n",
    "             if not seenp(m, robber_pos, cop_states, robber_states)]\n",
    "\n",
    "    return new_moves\n",
    "\n",
    "def get_closest_unoccupied(cop_pos: list, robber_pos: tuple, idx: int)-> tuple:\n",
    "    V = G.neighbors(cop_pos[idx])\n",
    "\n",
    "    min_dist = sys.maxsize\n",
    "    min_pos = cop_pos[idx]\n",
    "    \n",
    "    for v in V:\n",
    "        dist = math.sqrt((v[0] - robber_pos[0])**2 + (v[1] - robber_pos[1])**2)\n",
    "        if dist < min_dist and v not in cop_pos:\n",
    "            min_dist = dist\n",
    "            min_pos = v\n",
    "\n",
    "    return min_pos\n",
    "\n",
    "def minimize_avail_helper(curr_cop_pos, robber_pos, i, occupied_axes, cop_moves, robber_moves):\n",
    "    #goal is to find the minimizing config of cops\n",
    "    #so track a min config and min robber avail squares\n",
    "    #occupied axes is a dict, represents which robber axes are occupied in current backtracking iteration\n",
    "\n",
    "    #base case i > #cops\n",
    "    if i >= len(curr_cop_pos):\n",
    "        return curr_cop_pos, len(available_squares(curr_cop_pos, robber_pos))\n",
    "\n",
    "    avail = get_intersecting_squares(curr_cop_pos[i], robber_pos, occupied_axes)\n",
    "\n",
    "    moves = list() # list(posn) -> avail_squares\n",
    "\n",
    "    for move in avail:\n",
    "        #find axes this occupies\n",
    "        axis = get_axis(move, robber_pos)\n",
    "        occupied_axes[axis] = True\n",
    "        new_cop_pos = copy.deepcopy(curr_cop_pos)\n",
    "        new_cop_pos[i] = move #move cop i\n",
    "        \n",
    "        curr_config, curr_squares = minimize_avail_helper(new_cop_pos, robber_pos, i+1, occupied_axes, cop_moves, robber_moves)\n",
    "        moves.append((curr_config, curr_squares))\n",
    "    \n",
    "        occupied_axes[axis] = False\n",
    "\n",
    "    #remove all moves which revisit board states\n",
    "    moves = remove_seen_moves(moves, robber_pos, cop_moves, robber_moves)\n",
    "\n",
    "    if not moves:\n",
    "        #animal case-- go closer to the robber\n",
    "        curr_cop_pos[i] = get_closest_unoccupied(curr_cop_pos, robber_pos, i)\n",
    "        return minimize_avail_helper(curr_cop_pos, robber_pos, i+1, occupied_axes, cop_moves, robber_moves)\n",
    "    \n",
    "    #get squares with min # available squares for robber\n",
    "    vals = map(lambda tup: tup[1], moves)\n",
    "    min_val = min(vals)\n",
    "    min_avail_moves = list(filter(lambda tup: tup[1] == min_val, moves))\n",
    "\n",
    "    #sort by which config gives the max min SD\n",
    "    best_move = get_min_LSD_move(robber_pos, min_avail_moves)\n",
    "    \n",
    "    return best_move[0], best_move[1]\n",
    "\n",
    "def minimize_available(cop_pos: list, robber_pos: tuple, cop_states, robber_states) -> list:\n",
    "    # Function that returns the move for the cops that minimizes the number of available squares for the robber\n",
    "    #this function could be the combinatorially large one, but we are going to introduce our greedy heuristic\n",
    "    #our strategy is such: the cops should always directly threaten a unique line of movement\n",
    "    #get set of cop i available_moves \\intersect set of robber \n",
    "    #filter out whichever are on occupied axes\n",
    "    #use backtracking algorithm, recursively call min_avail_helper w/i+1, new cop_pos\n",
    "\n",
    "    occupied_axes = {\n",
    "        'h': False,\n",
    "        'v': False,\n",
    "        'negd': False,\n",
    "        'posd': False\n",
    "    }\n",
    "    \n",
    "    min_config, min_squares = minimize_avail_helper(cop_pos, robber_pos, 0, occupied_axes, cop_states, robber_states)\n",
    "    \n",
    "    return min_config\n",
    "\n",
    "def maximize_available(cop_pos: list, cop_states, robber_states, robber_pos:tuple = (-1, -1)) -> tuple: #-1 denotes no robber placed yet ie startin\n",
    "    # Function that returns move for the robber that maximizes the number of squares for their next turn (assuming cops try to minimize)\n",
    "    #get set of valid moves available_squares\n",
    "    #for all moves m, call available_squares(cop_pos, m), get size of set\n",
    "    #track max size and move, return that move\n",
    "    #O(n)\n",
    "\n",
    "    r_neighbors = available_squares(cop_pos, robber_pos)\n",
    "\n",
    "    moves = dict() # move -> min cop move in anticipation\n",
    "    \n",
    "    for move in r_neighbors:\n",
    "        #Q: here, do we want cop moves to take into account that cops wont repeat moves ?\n",
    "        #at this point, cops are making suboptimal moves\n",
    "        # i guess dont take into account, as robber doesnt care for repeating moves?\n",
    "        cop_response = minimize_available(cop_pos, move, [], [])\n",
    "        max_min_val = len(available_squares(cop_response, move))\n",
    "\n",
    "        moves[move] = max_min_val\n",
    "    \n",
    "    if not moves and robber_pos == (-1, -1):\n",
    "        return (0, 0)\n",
    "    elif not moves and robber_pos != (-1, -1):\n",
    "        return robber_pos\n",
    "\n",
    "    #sort by value descending, then by SD descending\n",
    "    max_val = max(moves.values())\n",
    "    max_avail_moves = {k: v for k, v in moves.items() if v == max_val}\n",
    "\n",
    "    best_moves = sorted(\n",
    "        max_avail_moves.items(),\n",
    "        key=lambda item: -get_SD_length(item[0])\n",
    "    )\n",
    "        \n",
    "    return best_moves[0][0]\n",
    "\n",
    "def k_cop_win(cop_start, robber_start, itr, cop_states, robber_states):\n",
    "    #returns true if cop win possible with k cops\n",
    "    cop_move = minimize_available(cop_start, robber_start, cop_states, robber_states) # The cops try to minimize the available squares\n",
    "    print(\"Cops move:\", cop_move)\n",
    "    cop_states.append(cop_move)\n",
    "    robber_states.append(robber_start)\n",
    "    avail_squares.append(len(available_squares(cop_move, robber_start)))\n",
    "    robber_move = maximize_available(cop_move, cop_states, robber_states, robber_start) # The robber tries to maximize this minimum\n",
    "    print(\"Robber moves to:\", robber_move)\n",
    "    print(avail_squares[-1], \"squares available for after move\", itr)\n",
    "\n",
    "    cop_states.append(cop_move)\n",
    "    robber_states.append(robber_move)\n",
    "    \n",
    "    # Checking if the cops have captured the robber\n",
    "    if len(available_squares(cop_move, robber_move)) == 0:\n",
    "        return True, cop_states, robber_states\n",
    "    \n",
    "    # If the cops can't decrease the number of available moves, they lose\n",
    "    #if len(avail_squares) > 1 and avail_squares[-1] > avail_squares[-2]:\n",
    "    #    print(\"available squares increased\")\n",
    "    #    return False\n",
    "\n",
    "    if itr > n**2:\n",
    "        print(\"iterations exceeded\")\n",
    "        return False, cop_states, robber_states\n",
    "            \n",
    "    # If the cop's haven't won yet, keep going\n",
    "    return k_cop_win(cop_move, robber_move, itr+1, cop_states, robber_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
<<<<<<< HEAD
   "id": "0869c5d5",
   "metadata": {},
=======
   "id": "12cf589d-a3cf-4cf6-96d7-e60ffa6d82cb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
>>>>>>> refs/remotes/origin/main
   "outputs": [],
   "source": [
    "#CODE FOR GENERATING ANIMAL/ROYAL GRAPHS GIVEN DIRECTIONS\n",
    "\n",
    "def make_graph(n, slopes, animal=False):\n",
    "    from sage.all import QQ, Infinity\n",
    "\n",
    "    vertices = [(x, y) for x in range(n) for y in range(n)]\n",
    "    G = Graph()\n",
    "    G.add_vertices(vertices)\n",
    "\n",
    "    for i, (x1, y1) in enumerate(vertices):\n",
    "        # Convert slope list to exact rational numbers or Infinity\n",
    "        D = set(QQ(s) if s != 'inf' else Infinity for s in slopes)\n",
    "        for j in range(i+1, len(vertices)):\n",
    "            x2, y2 = vertices[j]\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "\n",
    "            if dx == 0:\n",
    "                slope = Infinity\n",
    "            else:\n",
    "                slope = QQ(dy) / QQ(dx)\n",
    "\n",
    "            if slope in D:\n",
    "                G.add_edge((x1, y1), (x2, y2))\n",
    "                if animal:\n",
    "                    D.remove(slope)\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a884788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "EDIT THIS CODE TO CHANGE THE GRAPH\n",
    "similar to evans code, just input slopes into a list and pass it into make_graph function (specify animal or royal w/bool)\n",
    "'''\n",
    "\n",
    "n=6\n",
    "knight = [2, -2, 1/2, -1/2]\n",
    "queen = [0, 'inf', 1, -1]\n",
    "bishop = [1, -1]\n",
    "idk = [1/3, -1/3, 3, 3]\n",
    "G = make_graph(n, queen, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089929c6-b692-4fe1-a37c-f01727545e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[(3, 3), (4, 4), (2, 2)]\n",
      "rstart: (1, 0), cops: [(3, 3), (4, 4), (2, 2)]\n",
      "Cops move: [(1, 3), (4, 3), (0, 0)]\n",
      "Robber moves to: (1, 0)\n",
      "0 squares available for after move 1\n",
      "Cop win: True\n"
     ]
    }
   ],
   "source": [
    "#n = 15\n",
    "#T = n**2\n",
    "#G = graphs.QueenGraph([n,n])\n",
    "\n",
    "'''\n",
    "run this code to run the above greedy algorithm on the graph defined above\n",
    "define a list of tuples representing where you want your cops to start in (x,y) coords\n",
    "then pass into play_game()\n",
    "\n",
    "you could also use this to iteratively check the largest n for which k cops can win w/this algorithm in a loop\n",
    "'''\n",
    "\n",
    "avail_squares = list()\n",
    "\n",
    "def play_game(cops_start):\n",
    "    print(cops_start)\n",
    "    robber_start = maximize_available(cops_start.copy(), [], [])\n",
    "    robber_moves = [robber_start]\n",
    "    cop_moves = [cops_start]\n",
    "    print(f\"rstart: {robber_start}, cops: {cops_start}\")\n",
    "    return k_cop_win(cops_start, robber_start, 1, cop_moves, robber_moves)\n",
    "\n",
    "#6x6 domination\n",
    "dom_start = math.floor((n+1)/2) - 3\n",
    "dom_set = [(dom_start, dom_start), (dom_start + 4, dom_start + 2), (dom_start + 2, dom_start + 4)]\n",
    "\n",
    "corner_start = [(0,0), (n-1,n-1), (0,n-1)]\n",
    "\n",
    "two_cops = [(0,0), (n-1,n-1)]\n",
    "\n",
    "mid = math.floor(n/2)\n",
    "print(mid)\n",
    "four_cops = [(mid, mid), (mid-1, mid), (mid-1,mid-1), (mid,mid-1)]\n",
    "\n",
    "knight_diag = [(mid, mid), (mid+1, mid+1), (mid-1,mid-1)]\n",
    "\n",
    "winp, cop_moves, robber_moves = play_game(knight_diag)\n",
    "print(\"Cop win:\", winp)\n",
    "#print(cop_moves, robber_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28fde1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipyevents in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: ipywidgets>=7.6.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipyevents) (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (8.36.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipyevents) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (4.13.2)\n",
      "Requirement already satisfied: wcwidth in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->ipyevents) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipyevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9384dd-bc7e-4e5e-aa9a-a7266057b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.all import graphs\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "from ipyevents import Event\n",
    "\n",
    "\n",
    "def get_state(r_state, c_state):\n",
    "    #get dict w/red = occupied by cops, blue = cops, black = robber, green = available for robber movement\n",
    "    cop_occ = set()\n",
    "    for cop in c_state:\n",
    "        cop_occ = cop_occ.union(set(G.neighbors(cop)))\n",
    "\n",
    "    cop_occ -= set(c_state)\n",
    "    \n",
    "    state = {\n",
    "        'blue': set(c_state),\n",
    "        'black': {r_state},\n",
    "        'green': set(available_squares(c_state, r_state)) - {r_state},\n",
    "        'red': cop_occ - {r_state}\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def convert_to_state(robber_moves, cop_moves):\n",
    "    if len(robber_moves) != len(cop_moves):\n",
    "        raise Exception(\"nonequal lists given\")\n",
    "    \n",
    "    states = list()\n",
    "\n",
    "    for state in range(len(robber_moves)):\n",
    "        states.append(get_state(robber_moves[state], cop_moves[state]))\n",
    "\n",
    "    return states\n",
    "\n",
    "# Update function for each turn\n",
    "def update(turn, states, nx_G, pos):\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    nx.draw(nx_G, pos, ax=ax, node_color='lightgrey', node_size=400, with_labels=False)\n",
    "\n",
    "    for color, nodes in states[turn].items():\n",
    "        nx.draw_networkx_nodes(nx_G, pos, nodelist=list(nodes), node_color=color, node_size=400, ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Turn {math.floor(turn / 2) + 1}\")\n",
    "    ax.set_axis_off()\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "run this code to visualize moves made\n",
    "'''\n",
    "\n",
    "def display_game(G, cop_moves, robber_moves):\n",
    "    n = int(math.sqrt(len(list(G.vertices()))))\n",
    "\n",
    "    pos = {(i, j): (i, j) for i in range(n) for j in range(n)}\n",
    "    G.set_pos(pos)\n",
    "    nx_G = G.networkx_graph()\n",
    "\n",
    "    states = convert_to_state(robber_moves, cop_moves)\n",
    "\n",
    "    slider = widgets.IntSlider(min=0, max=len(states) - 1, step=1, value=0)\n",
    "    \n",
    "    out = widgets.interactive_output(\n",
    "        update,\n",
    "        {\n",
    "            'turn': slider,\n",
    "            'states': widgets.fixed(states),\n",
    "            'nx_G': widgets.fixed(nx_G),\n",
    "            'pos': widgets.fixed(pos)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    event = Event(source=slider, watched_events=['keydown'])\n",
    "\n",
    "    def handle_event(event):\n",
    "        if event['key'] == 'ArrowRight':\n",
    "            slider.value = min(slider.max, slider.value + slider.step)\n",
    "        elif event['key'] == 'ArrowLeft':\n",
    "            slider.value = max(slider.min, slider.value - slider.step)\n",
    "\n",
    "    event.on_dom_event(handle_event)\n",
    "\n",
    "    display(slider, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa566fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e49256a0c1749f8849c87d334e1d7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, max=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb85a3810ac409b919704777efa7134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_game(G, cop_moves, robber_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab9644",
   "metadata": {},
   "source": [
    "REINFORCEMENT LEARNING MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17529f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a653e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== classic_control =====\n",
      "Acrobot-v1             CartPole-v0            CartPole-v1\n",
      "MountainCar-v0         MountainCarContinuous-v0 Pendulum-v1\n",
      "===== phys2d =====\n",
      "phys2d/CartPole-v0     phys2d/CartPole-v1     phys2d/Pendulum-v0\n",
      "===== box2d =====\n",
      "BipedalWalker-v3       BipedalWalkerHardcore-v3 CarRacing-v3\n",
      "LunarLander-v3         LunarLanderContinuous-v3\n",
      "===== toy_text =====\n",
      "Blackjack-v1           CliffWalking-v0        FrozenLake-v1\n",
      "FrozenLake8x8-v1       Taxi-v3\n",
      "===== tabular =====\n",
      "tabular/Blackjack-v0   tabular/CliffWalking-v0\n",
      "===== mujoco =====\n",
      "Ant-v2                 Ant-v3                 Ant-v4\n",
      "Ant-v5                 HalfCheetah-v2         HalfCheetah-v3\n",
      "HalfCheetah-v4         HalfCheetah-v5         Hopper-v2\n",
      "Hopper-v3              Hopper-v4              Hopper-v5\n",
      "Humanoid-v2            Humanoid-v3            Humanoid-v4\n",
      "Humanoid-v5            HumanoidStandup-v2     HumanoidStandup-v4\n",
      "HumanoidStandup-v5     InvertedDoublePendulum-v2 InvertedDoublePendulum-v4\n",
      "InvertedDoublePendulum-v5 InvertedPendulum-v2    InvertedPendulum-v4\n",
      "InvertedPendulum-v5    Pusher-v2              Pusher-v4\n",
      "Pusher-v5              Reacher-v2             Reacher-v4\n",
      "Reacher-v5             Swimmer-v2             Swimmer-v3\n",
      "Swimmer-v4             Swimmer-v5             Walker2d-v2\n",
      "Walker2d-v3            Walker2d-v4            Walker2d-v5\n",
      "===== None =====\n",
      "GymV21Environment-v0   GymV26Environment-v0\n",
      "===== gymnasium_env =====\n",
      "gymnasium_env/CopsAndRobbers-v0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Dict, MultiDiscrete, Tuple, Discrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CopsAndRobbersEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for Cops and Robbers on an nxn grid or graph.\n",
    "    currently from the pov of the cops\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, k, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Inputs ---\n",
    "        self.k = k\n",
    "        self.graph = graph  #SageMath graph\n",
    "        self.nodes = list(self.graph.vertices())\n",
    "        self.n = math.sqrt(len(self.nodes))\n",
    "        max_deg = int(self.get_max_deg() + 1)\n",
    "\n",
    "        #print(type(max_deg))\n",
    "\n",
    "        # 1) Observation space: dict with\n",
    "        #    - \"cop_pos\": flattened k (x,y) pairs\n",
    "        #    - \"robber_pos\": single (x,y) pair\n",
    "        self.observation_space = Dict({\n",
    "            \"cop_pos\": MultiDiscrete([self.n, self.n] * self.k),   # [x1,y1, x2,y2, …, xk,yk]\n",
    "            \"robber_pos\": MultiDiscrete([self.n, self.n])     # [xr,yr]\n",
    "        })\n",
    "\n",
    "        # 2) Action space: a tuple of k Discrete spaces, each of size (max_deg+1)\n",
    "        #    (we’ll map 0…max_deg-1 to “move to the i‑th neighbor” and max_deg to “stay”)\n",
    "        self.action_space = Tuple([Discrete(max_deg) for _ in range(self.k)])\n",
    "\n",
    "        self.cop_pos = [(-1, -1)] * self.k #list of k tuples representing cop positions\n",
    "        self.robber_pos = (-1, -1)\n",
    "        self.render_mode = render_mode\n",
    "        self.itr = 0\n",
    "\n",
    "    def get_max_deg(self):\n",
    "        max_degree = 0\n",
    "\n",
    "        for vertex in self.graph.vertices():\n",
    "            degree = self.graph.degree(vertex)\n",
    "            if degree > max_degree:\n",
    "                max_degree = degree\n",
    "        return max_degree\n",
    "\n",
    "    def get_collective_euclidean_dist(self):\n",
    "        total_dist = 0\n",
    "        for cop in self.cop_pos:\n",
    "            total_dist += math.sqrt((cop[0] - self.robber_pos[0])**2 + (cop[1] - self.robber_pos[1])**2)\n",
    "        return total_dist\n",
    "    \n",
    "    '''\n",
    "    get the current observations, should be robber and cop positions i think\n",
    "    '''\n",
    "    def get_obs(self):\n",
    "        # Flatten your list of k (x,y) tuples into a single list:\n",
    "        flat_cops = [coord for pos in self.cop_pos for coord in pos]\n",
    "        # Robber is a single (x,y)\n",
    "        #print(self.robber_pos)\n",
    "        rob = list(self.robber_pos)\n",
    "        return {\n",
    "            \"cop_pos\": np.array(flat_cops, dtype=int),\n",
    "            \"robber_pos\": np.array(rob, dtype=int)\n",
    "        }\n",
    "    \n",
    "    def get_info(self):\n",
    "        '''\n",
    "        return info pertinent to reward\n",
    "        Q: what do we care about?\n",
    "        A: i think\n",
    "            - robber curr LSD length / layer (jacobs idea)\n",
    "            - #available squares for robber?\n",
    "        maybe define reward fn first\n",
    "        '''\n",
    "        return {\n",
    "            \"distance\": 1,\n",
    "            \"SD_length\": 1,\n",
    "            \"robber_available:\": 1\n",
    "        }\n",
    "\n",
    "    def reset(self, seed = None, options = None):       \n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state and return the initial observation.\n",
    "        random start?\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        #random for now, start doesnt matter\n",
    "        for _ in range(self.k):\n",
    "            self.cop_pos = random.sample(self.nodes, self.k)\n",
    "\n",
    "        self.robber_pos = maximize_available(self.cop_pos, [], []) #pick maximizing available start for robber\n",
    "        self.itr = 0\n",
    "        \n",
    "        # Return initial state\n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action (cop move) and return next observation, reward, terminated, truncated, info.\n",
    "        input is an action in the form of a k-tuple, each from 0 to max_deg\n",
    "        recall for cop alpha, a value of i means move to the ith neighbors, a value of max_deg means stay\n",
    "        \"\"\"\n",
    "        assert self.action_space.contains(action), \"Invalid action!\"\n",
    "\n",
    "        # Check terminal state first! to catch dominating start \n",
    "        terminated = not available_squares(self.cop_pos, self.robber_pos)\n",
    "        truncated = self.itr > max(self.n**2, 100)\n",
    "\n",
    "        if not terminated and not truncated: #make action for both cops and robbers\n",
    "            self.itr += 1\n",
    "    \n",
    "            for i, act in enumerate(action):\n",
    "                current_pos = self.cop_pos[i]\n",
    "                neighbors = self.graph.neighbors(current_pos)\n",
    "    \n",
    "                if act < len(neighbors):\n",
    "                    self.cop_pos[i] = neighbors[act]  #move to ith neighbor\n",
    "                else:\n",
    "                    pass  #stay in place\n",
    "    \n",
    "            #self.robber_pos = maximize_available(self.cop_pos, [], [], self.robber_pos)\n",
    "            choices = list(available_squares(self.cop_pos, self.robber_pos))\n",
    "            self.robber_pos = random.sample(choices, 1)[0] if choices else self.robber_pos\n",
    "    \n",
    "        # Define reward\n",
    "        reward = float()\n",
    "        if terminated:\n",
    "            reward = 5000 #reward for catching\n",
    "        elif truncated:\n",
    "            reward = -1000  #penalize running out of time\n",
    "        else:\n",
    "            #small negative reward each step to encourage faster capture\n",
    "            reward -= 0.1\n",
    "\n",
    "            #penalize robber mobility — less mobility is better for cops\n",
    "            mobility_penalty = len(available_squares(self.cop_pos, self.robber_pos))\n",
    "            reward -= 2.0 * mobility_penalty  # weight this — tune as needed\n",
    "\n",
    "            # Penalize collective distance from cops to robber\n",
    "            dist_penalty = self.get_collective_euclidean_dist()\n",
    "            reward -= 0.5 * dist_penalty  # again, weight can be tuned\n",
    "\n",
    "            #penalize cops on the same square\n",
    "            if len(self.cop_pos) != len(set(self.cop_pos)):\n",
    "                reward -= 100\n",
    "\n",
    "    \n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()        \n",
    "\n",
    "        #self.render()\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        visualize the environment\n",
    "        do it with above defined fns\n",
    "        \"\"\"\n",
    "        if self.render_mode == \"human\":\n",
    "            print(f\"Cops at {self.cop_pos}, Robber at {self.robber_pos}, iteration {self.itr}\")\n",
    "        \n",
    "    def get_moves(self):\n",
    "        return self.cop_pos, self.robber_pos\n",
    "\n",
    "gym.register(\n",
    "    id=\"gymnasium_env/CopsAndRobbers-v0\",\n",
    "    entry_point=CopsAndRobbersEnv,\n",
    ")\n",
    "gym.pprint_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e04981-9f9b-490b-b4f5-695f58d0f78b",
   "metadata": {
<<<<<<< HEAD
    "scrolled": true
=======
    "vscode": {
     "languageId": "python"
    }
>>>>>>> refs/remotes/origin/main
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4951.09607400234\n",
      "[(0, 1), (0, 4), (5, 4), (4, 3), (4, 3), (4, 3)]\n",
      "[[(2, 2), (5, 5), (1, 4)], [(1, 2), (5, 0), (4, 1)], [(1, 5), (0, 5), (4, 2)], [(0, 5), (0, 4), (1, 5)], [(0, 1), (2, 4), (4, 2)], [(0, 1), (2, 4), (4, 2)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/envs/registration.py:736: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='human' that is not in the possible render_modes ([]).\u001b[0m\n",
      "  logger.warn(\n",
      "/home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'sage.rings.real_mpfr.RealNumber'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3, render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "total_reward = 0.0\n",
    "c_states = list()\n",
    "r_states = list()\n",
    "\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample() # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "    cop_state, robber_state = env.unwrapped.get_moves()\n",
    "    #print(c_states)\n",
    "    c_states.append(copy.deepcopy(cop_state))\n",
    "    r_states.append(copy.deepcopy(robber_state))\n",
    "\n",
    "print(total_reward)\n",
    "print(r_states)\n",
    "print(c_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "611f3d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6222818356334a0e91a5d7569afd6255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, max=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78effa45b62e4e8faa2ef86a9c24b17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_game(G, c_states, r_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21d291f3-828f-47b8-b524-19350a49039a",
   "metadata": {
<<<<<<< HEAD
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
=======
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from gymnasium.spaces import Tuple, MultiDiscrete\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # force CPU\n",
    "\n",
    "class TupleToMultiDiscreteWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, Tuple)\n",
    "        self.original_space = env.action_space\n",
    "        self.action_space = MultiDiscrete([space.n for space in env.action_space])\n",
    "\n",
    "    def action(self, action):\n",
    "        return tuple(action)\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        return list(action)\n",
    "\n",
    "def make_env(G):\n",
    "    def _init():\n",
    "        env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3)\n",
    "        env = TupleToMultiDiscreteWrapper(env)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a36436-91c3-4763-9ee4-86394a49930b",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
>>>>>>> refs/remotes/origin/main
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'sage.rings.real_mpfr.RealNumber'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.3     |\n",
      "|    ep_rew_mean     | 286      |\n",
      "| time/              |          |\n",
      "|    fps             | 844      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35.6        |\n",
      "|    ep_rew_mean          | 380         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 549         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005884357 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.98       |\n",
      "|    explained_variance   | 1.11e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.13e+05    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 1.54e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.3         |\n",
      "|    ep_rew_mean          | 281          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 488          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034624797 |\n",
      "|    clip_fraction        | 0.0038       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.98        |\n",
      "|    explained_variance   | -0.00025     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.34e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00883     |\n",
      "|    value_loss           | 1.46e+06     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 36.4      |\n",
      "|    ep_rew_mean          | -239      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 464       |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 141       |\n",
      "|    total_timesteps      | 65536     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0035047 |\n",
      "|    clip_fraction        | 0.00536   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.98     |\n",
      "|    explained_variance   | -0.000182 |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.65e+06  |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -0.00903  |\n",
      "|    value_loss           | 1.41e+06  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.3         |\n",
      "|    ep_rew_mean          | 486          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 453          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034350134 |\n",
      "|    clip_fraction        | 0.00527      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.97        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0091      |\n",
      "|    value_loss           | 1.45e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 34.8         |\n",
      "|    ep_rew_mean          | 393          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 449          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 218          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022136653 |\n",
      "|    clip_fraction        | 0.00115      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.97        |\n",
      "|    explained_variance   | 0.000157     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99e+06     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00715     |\n",
      "|    value_loss           | 1.54e+06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 273         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 445         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002896532 |\n",
      "|    clip_fraction        | 0.00238     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.97       |\n",
      "|    explained_variance   | 0.000161    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.16e+04    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00715    |\n",
      "|    value_loss           | 1.26e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.5        |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 443         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003307229 |\n",
      "|    clip_fraction        | 0.0038      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.96       |\n",
      "|    explained_variance   | -0.000342   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.49e+05    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00851    |\n",
      "|    value_loss           | 2.11e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.1         |\n",
      "|    ep_rew_mean          | 287          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 444          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 332          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022203065 |\n",
      "|    clip_fraction        | 0.000726     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.96        |\n",
      "|    explained_variance   | -0.000322    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.27e+05     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    value_loss           | 1.17e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.3         |\n",
      "|    ep_rew_mean          | 481          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 375          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024693434 |\n",
      "|    clip_fraction        | 0.000934     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.96        |\n",
      "|    explained_variance   | -2.98e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.83e+05     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00628     |\n",
      "|    value_loss           | 1.26e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 34.9         |\n",
      "|    ep_rew_mean          | 387          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 412          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016822489 |\n",
      "|    clip_fraction        | 0.000208     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.95        |\n",
      "|    explained_variance   | 7.16e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.04e+05     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    value_loss           | 1.9e+06      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.3         |\n",
      "|    ep_rew_mean          | -133         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 433          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 453          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012617433 |\n",
      "|    clip_fraction        | 3.66e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.95        |\n",
      "|    explained_variance   | 2.74e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.56e+06     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00463     |\n",
      "|    value_loss           | 1.73e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.9         |\n",
      "|    ep_rew_mean          | 273          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 431          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 494          |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026042839 |\n",
      "|    clip_fraction        | 0.000806     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.95        |\n",
      "|    explained_variance   | -0.000805    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.58e+05     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00506     |\n",
      "|    value_loss           | 1.07e+06     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "G = make_graph(6, queen, False)\n",
    "\n",
    "# Create vectorized environment with, say, 8 parallel copies\n",
    "env = make_vec_env(make_env(), n_envs=8)\n",
    "#env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3, render_mode=\"human\")\n",
    "#env = TupleToMultiDiscreteWrapper(env)\n",
    "\n",
    "# Train PPO\n",
    "model = PPO(\"MultiInputPolicy\", env, device='cuda', verbose=1)\n",
    "model.learn(total_timesteps=200_000)\n",
    "\n",
    "model.save(\"ppo_cr_v1.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a97aa0f",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (1.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.2.6)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.7.0)\n",
      "Requirement already satisfied: cloudpickle in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: pandas in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (3.10.3)\n",
      "Requirement already satisfied: opencv-python in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (4.11.0.86)\n",
      "Requirement already satisfied: pygame in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.19.0)\n",
      "Requirement already satisfied: psutil in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (4.67.1)\n",
      "Requirement already satisfied: rich in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (14.0.0)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: pillow in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from stable-baselines3[extra]) (11.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: filelock in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from triton==3.3.0->torch<3.0,>=2.3->stable-baselines3[extra]) (80.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.8)\n",
      "Requirement already satisfied: packaging in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (6.31.0)\n",
      "Requirement already satisfied: six>1.9 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/daniel/miniconda3/envs/sage/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "0aaaac9a",
   "metadata": {},
=======
   "execution_count": 32,
   "id": "90adf1cf-964b-4701-82e3-1e061f98f8cb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
>>>>>>> refs/remotes/origin/main
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False  True False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False  True False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False  True False]\n",
      "[False False False False False False False False]\n",
      "[False False False  True False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False False False False False False False False]\n",
      "[False  True False False False False False False]\n",
      "[ True False False False False False False False]\n",
      "total reward: 4501.26318359375, number of rounds: 32\n",
      "Mean reward: 3170.9480998000004 ± 2833.3871553058675\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Tuple, MultiDiscrete, Discrete\n",
    "\n",
    "G = make_graph(8, queen, False)\n",
    "\n",
    "# Wrap your env\n",
    "#env = make_vec_env(lambda: gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3), n_envs=1)\n",
    "env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3, render_mode=\"human\")\n",
    "\n",
    "class TupleToMultiDiscreteWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, Tuple)\n",
    "        self.original_space = env.action_space\n",
    "        self.action_space = MultiDiscrete([space.n for space in env.action_space])\n",
    "\n",
    "    def action(self, action):\n",
    "        # Converts array back to tuple\n",
    "        return tuple(action)\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        return list(action)\n",
    "\n",
    "# Usage:\n",
    "env = TupleToMultiDiscreteWrapper(env)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "model.save(\"ppo_cops_and_robbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e3225da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0650000000000000\n",
      "[(2, 7), (3, 7), (7, 3), (7, 2), (3, 2), (3, 1), (5, 1), (0, 1), (1, 1), (4, 1), (4, 4), (0, 0), (2, 2), (2, 2), (2, 0), (2, 2), (4, 4), (5, 4), (3, 4), (3, 0), (6, 0), (6, 3), (6, 6), (3, 6), (3, 3), (1, 5), (1, 7), (6, 2), (6, 5), (2, 1), (5, 4), (7, 6), (7, 2)]\n",
      "[[(4, 6), (5, 6), (0, 0)], [(4, 3), (5, 0), (4, 0)], [(1, 6), (0, 0), (5, 0)], [(4, 3), (0, 0), (0, 0)], [(4, 7), (0, 0), (7, 0)], [(7, 4), (0, 7), (6, 0)], [(4, 7), (0, 7), (3, 0)], [(4, 6), (1, 6), (3, 7)], [(6, 4), (5, 6), (3, 7)], [(6, 0), (5, 5), (0, 7)], [(6, 0), (7, 5), (1, 6)], [(6, 5), (7, 5), (3, 6)], [(0, 5), (7, 4), (4, 5)], [(0, 1), (5, 6), (4, 1)], [(6, 1), (0, 1), (6, 3)], [(6, 1), (0, 1), (6, 7)], [(0, 1), (0, 2), (5, 7)], [(6, 7), (0, 3), (0, 2)], [(1, 7), (0, 3), (0, 2)], [(4, 4), (0, 1), (0, 2)], [(5, 5), (0, 4), (0, 4)], [(5, 0), (2, 4), (2, 2)], [(2, 3), (2, 4), (7, 2)], [(2, 1), (0, 4), (2, 2)], [(6, 1), (0, 4), (2, 6)], [(7, 1), (0, 0), (2, 2)], [(2, 1), (0, 4), (3, 1)], [(1, 0), (0, 4), (3, 4)], [(3, 0), (4, 4), (0, 7)], [(6, 0), (3, 5), (3, 7)], [(6, 2), (3, 7), (3, 7)], [(6, 2), (0, 4), (4, 7)], [(6, 5), (0, 4), (4, 7)]]\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make(\"gymnasium_env/CopsAndRobbers-v0\", graph=G, k=3, render_mode=\"human\")\n",
    "model = PPO.load(\"ppo_cops_and_robbers\")\n",
    "\n",
=======
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "model = PPO.load('/mnt/c/Users/danie/reu2025/reu2025/ppo_cr_v1', device='cpu')\n",
>>>>>>> refs/remotes/origin/main
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "c_states = list()\n",
    "r_states = list()\n",
    "\n",
    "G = make_graph(6, queen, False)\n",
    "\n",
    "env = make_vec_env(make_env(G), n_envs=8)\n",
    "obs = env.reset()\n",
    "cop_state, robber_state = env.envs[0].unwrapped.get_moves()\n",
    "c_states.append(copy.deepcopy(cop_state))\n",
    "r_states.append(copy.deepcopy(robber_state))\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "itr = 0\n",
    "\n",
    "while not done:\n",
    "    itr += 1\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, infos = env.step(action)\n",
    "    total_reward += rewards[0]      # since vectorized you get a 1‑element batch\n",
    "    done = dones[0]\n",
    "    print(dones)\n",
    "\n",
    "    if done: #lmao\n",
    "        break\n",
    "\n",
    "    cop_state, robber_state = env.envs[0].unwrapped.get_moves()\n",
    "    c_states.append(copy.deepcopy(cop_state))\n",
    "    r_states.append(r_states[-1])\n",
    "\n",
    "    c_states.append(c_states[-1])\n",
    "    r_states.append(copy.deepcopy(robber_state))\n",
    "\n",
    "print(f\"total reward: {total_reward}, number of rounds: {itr}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882df629-e63d-48a7-9df6-667deca2921b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71440e74f051475ea71833d0e65d13c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, max=62)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1301eb649c4a02a503e928ada71bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_game(G, c_states, r_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.6",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "sage",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
